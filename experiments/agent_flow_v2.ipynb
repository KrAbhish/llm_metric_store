{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `AzureOpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureOpenAIEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_community/embeddings/azure_openai.py:113: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://polarisopenai.openai.azure.com/ to https://polarisopenai.openai.azure.com//openai.\n",
      "  warnings.warn(\n",
      "/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_community/embeddings/azure_openai.py:120: UserWarning: As of openai>=1.0.0, if `deployment` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_community/embeddings/azure_openai.py:128: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://polarisopenai.openai.azure.com/ to https://polarisopenai.openai.azure.com//openai.\n",
      "  warnings.warn(\n",
      "/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_community/chat_models/azure_openai.py:167: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://polariseastus2.openai.azure.com/ to https://polariseastus2.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_community/chat_models/azure_openai.py:174: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_community/chat_models/azure_openai.py:182: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://polariseastus2.openai.azure.com/ to https://polariseastus2.openai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.utils.llamaindex_retriever import LlamaIndexRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.azure_openai import AzureOpenAIEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def load_env_variables(file_path):\n",
    "    load_dotenv(file_path)\n",
    "    print(\"Environment variables loaded successfully!\")\n",
    "\n",
    "env_file_path = \"../.env\"\n",
    "load_env_variables = load_env_variables(env_file_path)\n",
    "max_tokens = 3500\n",
    "temperature = 0.1\n",
    "\n",
    "# embeddings = AzureOpenAIEmbeddings(azure_deployment=azure_deployment, openai_api_version=openai_api_version)\n",
    "embeddings =  AzureOpenAIEmbeddings(\n",
    "        deployment=os.getenv(\"EMB_DEPLOYMENT\"),\n",
    "        openai_api_version=os.getenv(\"EMB_OPENAI_API_VERSION\"),\n",
    "        model=os.getenv(\"EMB_MODEL\"),\n",
    "        openai_api_key=os.getenv(\"EMB_OPENAI_API_KEY\"),\n",
    "        openai_api_base=os.getenv(\"EMB_OPENAI_ENDPOINT\"),\n",
    "        openai_api_type=os.getenv(\"EMB_API_TYPE\"),\n",
    "    )\n",
    "\n",
    "llm_service = AzureChatOpenAI(deployment_name=os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME'), openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "                        openai_api_base=os.getenv(\"OPENAI_API_BASE\"), \n",
    "                        openai_api_type= os.getenv(\"OPENAI_API_TYPE\"),\n",
    "                        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                        max_tokens=max_tokens,\n",
    "                        temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define utils\n",
    "from src.utils.cube_semantic_custom import CubeSemanticLoader\n",
    "def fetch_cube_metadata(*args, **kwargs):\n",
    "    try:\n",
    "        # # Load document from Cube meta api\n",
    "        loader = CubeSemanticLoader(os.getenv(\"CUBE_API_URL\"), os.getenv(\"CUBE_TOKEN\"), False)\n",
    "        documents = loader.load()\n",
    "        # to_json()\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        # Handle exceptions gracefully and return an error response\n",
    "        print(\"Error in fetching metadata from cube: \" + str(e))\n",
    "        return 0\n",
    "\n",
    "def create_vector_store(documents, local_vector_store_path, *args, **kwargs):\n",
    "    print(\"Loaded documents: \" + str(documents))\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    vectorstore.save_local(local_vector_store_path)\n",
    "    print(\"Vector store created and saved successfully!\")\n",
    "\n",
    "def load_vector_store(vector_store_path, embeddings, *args, **kwargs):\n",
    "    # Load the vector store from the local file system\n",
    "    vectorstore = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"Vector store loaded successfully!\")\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#load existing vector store\n",
    "vector_store_path = \"/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/vector_store/cube_meta_faiss_index\"\n",
    "vectorstore = load_vector_store(vector_store_path, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from crewai import Agent, Task, Crew\n",
    "\n",
    "from crewai.telemetry import Telemetry\n",
    "\n",
    "def noop(*args, **kwargs):\n",
    "    print(\"Telemetry method called and noop'd\\n\")\n",
    "    pass\n",
    "for attr in dir(Telemetry):\n",
    "    if callable(getattr(Telemetry, attr)) and not attr.startswith(\"__\"):\n",
    "        setattr(Telemetry, attr, noop)\n",
    "\n",
    "os.environ[\"OTEL_SDK_DISABLED\"] = \"true\"\n",
    "\n",
    "from langchain.tools import tool\n",
    "from src.utils.llamaindex_retriever import LlamaIndexRetriever\n",
    "from typing import Optional, Type\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "# Import things that are needed generically\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "# from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from crewai_tools import BaseTool\n",
    "\n",
    "def get_similar_documents_faiss(query, max_number_documents=3):\n",
    "  vectorstore = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n",
    "  docs = vectorstore.similarity_search_with_relevance_scores(query, max_number_documents)\n",
    "  relevant_documents = []\n",
    "  for doc in docs:\n",
    "      doc = doc[0]\n",
    "      meta = {'text':doc.page_content, 'table_metadata': doc.metadata}\n",
    "      relevant_documents.append(meta)\n",
    "  return relevant_documents\n",
    "\n",
    "def get_similar_documents(query, max_number_documents=3):\n",
    "    return get_similar_documents_faiss(query, max_number_documents)\n",
    "\n",
    "\n",
    "class QueryInput(BaseModel):\n",
    "    query: str = Field(description=\"should be enquiry query\")\n",
    "\n",
    "class RephraseInputQuery(BaseTool):\n",
    "    name:str = \"rephrase_input_query\"\n",
    "    description :str = \"Useful to rephrase the query to capture the intent of the user regarding metric information\"\n",
    "    args_schema: Type[BaseModel] = QueryInput\n",
    "\n",
    "    def _run(\n",
    "        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        metric_description = self.rephrase_input_query(query)\n",
    "        return metric_description\n",
    "\n",
    "    async def _arun(\n",
    "        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"custom_search does not support async\")\n",
    "    \n",
    "    \n",
    "    def rephrase_input_query(self, query, *args, **kwargs):\n",
    "        agent = Agent(\n",
    "                role='Intent Capturer',\n",
    "                goal=\n",
    "                'Rephrasing the query to capture the intent of the user regarding metric information',\n",
    "                backstory=\n",
    "                \"You are an expert to understand the user's intent and rephrase the query to capture the intent of the user accurately.\",\n",
    "                llm = llm_service,\n",
    "                allow_delegation=False)\n",
    "        task = Task(\n",
    "                agent=agent,\n",
    "                description=\n",
    "                f'Rephrase the query to capture the intent of the user regarding metric information. The query is {query}. Donot add any noise to the response',\n",
    "                expected_output=\"some string\",\n",
    "            )\n",
    "        extracted_metrics = task.execute()\n",
    "\n",
    "        return extracted_metrics\n",
    "    \n",
    "\n",
    "    \n",
    "class MetricDiscovery(BaseTool):\n",
    "    name :str = \"metric_discovery\"\n",
    "    description:str = \"\"\"Useful for general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns.\"\"\"\n",
    "    args_schema: Type[BaseModel] = QueryInput\n",
    "\n",
    "    def _run(\n",
    "        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        metric_description = self.metric_discovery(query)\n",
    "        return metric_description\n",
    "\n",
    "    async def _arun(\n",
    "        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"custom_search does not support async\")\n",
    "    \n",
    "    \n",
    "    def metric_discovery(self, query, *args, **kwargs):\n",
    "        \"\"\"Useful for general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns.\"\"\"\n",
    "        relevant_documents = get_similar_documents(query)\n",
    "        agent = Agent(\n",
    "                role='Data Analyst Assistant',\n",
    "                goal=\n",
    "                'Empower users to understand and utilize data effectively. This includes helping them discover relevant metrics, interpreting their meaning',\n",
    "                backstory=\n",
    "                \"The primary purpose is to bridge the gap between raw data and user comprehension, fostering a data-driven culture within the organization.\",\n",
    "                llm = llm_service,\n",
    "                allow_delegation=False)  \n",
    "        \n",
    "        task = Task(\n",
    "                agent=agent,\n",
    "                description=\n",
    "                \"\"\" You are responding to  question {metric_description} with answer from the Metadata provided to you as {relevant_documents}}.\n",
    "                    Strictly answer the question with the information present in metadata only.\n",
    "                    Respond with \"Sorry, the query is out of scope.\" if the answer is not present in metadata and terminate further reasoning and prcoess.\n",
    "                    \"\"\",\n",
    "                expected_output=\"some string\",\n",
    "            )\n",
    "        output = task.execute()\n",
    "\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricDiscoveryTasks():\n",
    "  def metric_isolation(self, agent, query):\n",
    "    return Task(description=(f\"\"\"\n",
    "        Rephrase the query to capture the intent of the user regarding metric information. The query is {query}. Donot add any noise to the response.\n",
    "        {self.__tip_section()}\"\"\"),\n",
    "      agent=agent,\n",
    "      expected_output=\"Reformatted query to capture the intent of the user regarding metric information.\",\n",
    "    )\n",
    "  \n",
    "  def metric_discovery(self, agent):\n",
    "    return Task(description=(f\"\"\"\n",
    "        Answer the general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns.\n",
    "        {self.__tip_section()}\"\"\"),\n",
    "      agent=agent,\n",
    "      expected_output=\"If relevant metric exists in metadata, provide the answer. Else, respond with 'Sorry, the query is out of scope.' and terminate process.\",\n",
    "    )\n",
    "  def __tip_section(self):\n",
    "    return \"If you do your BEST WORK, I'll give you a $10,000 commission!\"\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricDiscoveryAgent():\n",
    "  def user_intent_capture(self):\n",
    "    return Agent(\n",
    "      role='Intent Capturer',\n",
    "      goal=\n",
    "      'Rephrasing the query to capture the intent of the user regarding metric information',\n",
    "      backstory=\n",
    "      \"You are an expert to understand the user's intent and rephrase the query to capture the intent of the user accurately.\",\n",
    "      verbose=True,\n",
    "      tools=[\n",
    "        RephraseInputQuery()\n",
    "      ],\n",
    "      llm = llm_service,\n",
    "    )\n",
    "  def discover_metric_info(self):\n",
    "    return Agent(\n",
    "     role='Data Analyst Assistant',\n",
    "      goal=\n",
    "      'Empower users to understand and utilize data effectively. This includes helping them discover relevant metrics, interpreting their meaning',\n",
    "      backstory=\n",
    "      \"The primary purpose is to bridge the gap between raw data and user comprehension, fostering a data-driven culture within the organization.\",\n",
    "      llm = llm_service,\n",
    "      verbose=True,\n",
    "      tools=[\n",
    "        MetricDiscovery()\n",
    "      ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class Expert(ABC):\n",
    "    name: str = \"\"\n",
    "    description: str = \"\"\n",
    "    public_description: str = \"\"\n",
    "    arg_description: str = \"The argument to the function.\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def call(\n",
    "        self,\n",
    "        query: str,\n",
    "    ) -> str:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expert definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.process import Process\n",
    "class MetricDiscoveryInputCrew(Expert):\n",
    "  name = \"MetricDiscovery\"\n",
    "  description = (\n",
    "       \"Use this to answer the general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns.\"\n",
    "    )\n",
    "  public_description = \"Metric discovery crew\"\n",
    "  arg_description = \"Query related to metrics.\"\n",
    "\n",
    "  def crew_process(self, query: str):\n",
    "    agents = MetricDiscoveryAgent()\n",
    "    tasks = MetricDiscoveryTasks()\n",
    "    # print(agents)\n",
    "    # print(tasks)\n",
    "    user_intent_capture_agent = agents.user_intent_capture()\n",
    "    discover_metric_info_agent = agents.discover_metric_info()\n",
    "    # print(metric_isolator_agent)\n",
    "    # metric_isolator_task = tasks.metric_isolation(metric_isolator_agent, self.query)\n",
    "    metric_isolator_task = tasks.metric_isolation(user_intent_capture_agent, query)\n",
    "    metric_discover_task = tasks.metric_discovery(discover_metric_info_agent)\n",
    "    \n",
    "    # print(\"Metric_isolator_task\", metric_isolator_task)\n",
    "    crew = Crew(\n",
    "      agents=[\n",
    "        user_intent_capture_agent,\n",
    "        discover_metric_info_agent,\n",
    "      ],\n",
    "      tasks=[\n",
    "        metric_isolator_task,\n",
    "        metric_discover_task\n",
    "      ],\n",
    "      verbose=False,\n",
    "      process=Process.sequential,\n",
    "    )\n",
    "    return crew\n",
    "  \n",
    "\n",
    "  def run(self, query: str):\n",
    "    crew = self.crew_process(query)\n",
    "    result = crew.kickoff()\n",
    "    return result\n",
    "  \n",
    "  def call(\n",
    "        self, query: str, *args: Any, **kwargs: Any\n",
    "    ) -> str:\n",
    "    crew = self.crew_process(query)\n",
    "    result = crew.kickoff()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Available experts\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from typing import Type, TypedDict\n",
    "\n",
    "class ExpertDescription(TypedDict):\n",
    "    \"\"\"Representation of a callable expert\"\"\"\n",
    "    name: str\n",
    "    \"\"\"The name of the expert.\"\"\"\n",
    "    description: str\n",
    "    \"\"\"A description of the expert.\"\"\"\n",
    "    parameters: dict[str, object]\n",
    "    \"\"\"The parameters of the expert.\"\"\"\n",
    "\n",
    "def get_expert_function(expert: Type[Expert]) -> ExpertDescription:\n",
    "    \"\"\"A function that will return the tool's function specification\"\"\"\n",
    "    name = get_expert_name(expert)\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"description\": expert.description,\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"reasoning\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": (\n",
    "                        f\"Reasoning is how the task will be accomplished with the current expert. \"\n",
    "                        \"Detail your overall plan along with any concerns you have.\"\n",
    "                        \"Ensure this reasoning value is in the user defined langauge \"\n",
    "                    ),\n",
    "                },\n",
    "                \"arg\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": expert.arg_description,\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"reasoning\", \"arg\"],\n",
    "        },\n",
    "    }\n",
    "\n",
    "def get_expert_name(expert: Type[Expert]) -> str:\n",
    "    return expert.name\n",
    "\n",
    "# def format_expert_name(expert_name: str) -> str:\n",
    "#     return expert_name.lower()\n",
    "\n",
    "def get_available_experts() -> List[Type[Expert]]:\n",
    "    return [\n",
    "        MetricDiscoveryInputCrew\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query = \"What is the most popular feature used by our paid subscribers?\"\n",
    "crew = MetricDiscoveryInputCrew()\n",
    "result = crew.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query breakdown into objectives/intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "def create_sub_objectives_1(query):\n",
    "    # Define the prompt template\n",
    "    prompt_create_objectives = PromptTemplate(\n",
    "        input_variables=[\"query\"],\n",
    "        template=\"\"\"\n",
    "    You are an intelligent assistant that helps users break down their goals into high-level, smaller objectives. The system you are part of serves as an interface between users and a database containing various metrics. You have access to metadata of these metrics, including table columns information.\n",
    "    Goal: {query}\n",
    "    Using the provided goal and any relevant chat history, break down the goal into high-level, smaller objectives whose intents are different. Make sure each objective is clear and distinct.\n",
    "    ) \"\"\")\n",
    "    prompt = prompt_create_objectives.format_prompt(query=query,)\n",
    "    response = llm_service.invoke(str(prompt))\n",
    "    return response.content\n",
    "\n",
    "#############################################################################################################################################################################################\n",
    "def complete(prompt, user_query):\n",
    "    \"\"\"\n",
    "    Completes the prompt with the user objective.\n",
    "\n",
    "    Args:\n",
    "        prompt: The prompt template.\n",
    "        user_objective: The user's objective.\n",
    "\n",
    "    Returns:\n",
    "        The completed prompt.\n",
    "    \"\"\"\n",
    "    return prompt.format(query=user_query)\n",
    "\n",
    "def create_sub_objectives_2(query):\n",
    "    prompt_objective_breakdwon = PromptTemplate.from_template(\"\"\"\n",
    "    You are an intelligent assistant whose only and only goal is to break down user's query {query} into high-level, smaller objectives in form of list. You serve as an interface between users and a database containing various metrics/KPIs/measures.\n",
    "    Strictly note that it is not necessary to break down the query in all the following intents. The intents are only for reference. If the query is straightforward and aligns with any one of the intent, then do not change the query. \n",
    "    Try to minimize the number of objectives and keep them as high-level as possible. \n",
    "    Each smaller objective should strictly fall into one of the following intent:\n",
    "    Discover Metrics:** If you'd like to measure something specific, I can identify relevant metrics. For example, are you interested in suggest some metrics based on objective?\n",
    "    Fetch Data:** If you already know the metric you're interested in, I can retrieve the data from the database.\n",
    "    Interpret Metrics:** Once you have the data, I can explain what it means in the context of your objective. \n",
    "    Understand Metrics:** If you're unsure about a metric's definition or purpose, I can provide more information.\n",
    "\n",
    "    Don't add noise to the response.\n",
    "    Respond with a list.\n",
    "    However, if the query involves something else entirely, like action outside current intents, give empty list.\"\"\")\n",
    "    completed_prompt = complete(prompt_objective_breakdwon, query)\n",
    "    response = llm_service.invoke(completed_prompt)\n",
    "    return response.content\n",
    "\n",
    "#############################################################################################################################################################################################\n",
    "intents = {\n",
    "    \"discover_metrics\": \"Identify relevant metrics based on user objectives. (e.g., Suggest metrics to measure customer satisfaction)\",\n",
    "    \"fetch_metric\": \"Retrieve data for a specific metric the user has already identified.\",\n",
    "    # \"analyze_trends\": \"Analyze trends within the fetched data to provide deeper insights.\",\n",
    "    \"interpret_metrics\": \"Explain the meaning of the data in the context of the user's objective.\",\n",
    "    \"understand_metrics\": \"Provide information about a metric's definition and purpose.\"\n",
    "}\n",
    "def create_sub_objectives_3(query):\n",
    "    prompt_objective_breakdwon = PromptTemplate.from_template(\"\"\"You are an intelligent assistant focused on helping users with metrics and data. Your primary goal is to break down user queries ({query}) into a concise list of high-level objectives.\n",
    "    **Intents:**\n",
    "\n",
    "    {intents}\n",
    "\n",
    "    Important Notes:\n",
    "    * Alignment with Intents: The new objectives should strictly align with the provided intents.\n",
    "    * Prioritize Clarity & Efficiency:  Minimize the number of objectives and maintain a high-level overview. \n",
    "    * Direct Match: If the query aligns perfectly with one intent (e.g., \"What's the current customer churn rate?\" for Fetch Data), don't alter it.\n",
    "    * Outside Scope: If the query falls outside these intents (e.g., \"Book a flight\"), return an empty list.\n",
    "    * Focus on User Need: Keep responses clear and concise, avoiding unnecessary information. \n",
    "    * Output Format:  Respond with a list containing the breakdown of objectives.\n",
    "\n",
    "    **Here's the user query: {query}\n",
    "\n",
    "    What is the high-level breakdown of objectives for this query?\n",
    "    \"\"\")\n",
    "\n",
    "    # Format the intent descriptions for the prompt\n",
    "    formatted_intents = \"\\n\".join([f\"* {intent_name}: {description}\" for intent_name, description in intents.items()])\n",
    "    query = \"What dose the vessel name column represents?\"\n",
    "    # Insert formatted intents into the prompt template\n",
    "    completed_prompt = prompt_objective_breakdwon.format(intents=formatted_intents, query=query)\n",
    "    # Use the completed prompt with the AzureChatOpenAI model\n",
    "    response = llm_service.invoke(completed_prompt)\n",
    "    return response.content   \n",
    "\n",
    "#############################################################################################################################################################################################\n",
    "\n",
    "def create_sub_objectives(query):\n",
    "    response = create_sub_objectives_3(query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from typing import List, TypeVar\n",
    "from langchain.schema import BaseOutputParser, OutputParserException\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "class TaskOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"\n",
    "    Extension of LangChain's BaseOutputParser\n",
    "    Responsible for parsing task creation output into a list of task strings\n",
    "    \"\"\"\n",
    "\n",
    "    tasks: List[str] = []\n",
    "\n",
    "    def __init__(self, *, tasks: List[str]):\n",
    "        super().__init__()\n",
    "        self.tasks = tasks\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Parses the task creation output into a list of task strings.\n",
    "\n",
    "        Args:\n",
    "            text (str): The task creation output text.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The list of parsed task strings.\n",
    "\n",
    "        Raises:\n",
    "            OutputParserException: If parsing fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            array_str = extract_array(text)\n",
    "            all_tasks = [\n",
    "                remove_prefix(task) for task in array_str if real_tasks_filter(task)\n",
    "            ]\n",
    "            return [task for task in all_tasks if task not in self.tasks]\n",
    "        except Exception as e:\n",
    "            msg = f\"Failed to parse tasks from completion '{text}'. Exception: {e}\"\n",
    "            raise OutputParserException(msg)\n",
    "\n",
    "    def get_format_instructions(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns the format instructions for the task output.\n",
    "\n",
    "        Returns:\n",
    "            str: The format instructions.\n",
    "        \"\"\"\n",
    "        return \"\"\"\n",
    "        The response should be a JSON array of strings. Example:\n",
    "\n",
    "        [\"Search the web for NBA news\", \"Write some code to build a web scraper\"]\n",
    "\n",
    "        This should be parsable by json.loads()\n",
    "        \"\"\"\n",
    "\n",
    "def handle_multiline_string(input_str: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Handles a multiline string as a list.\n",
    "\n",
    "    Args:\n",
    "        input_str (str): The input multiline string.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: The processed lines as a list of strings.\n",
    "    \"\"\"\n",
    "    processed_lines = [\n",
    "        re.sub(r\".*?(\\d+\\..+)\", r\"\\1\", line).strip()\n",
    "        for line in input_str.split(\"\\n\")\n",
    "        if line.strip() != \"\"\n",
    "    ]\n",
    "\n",
    "    if any(re.match(r\"\\d+\\..+\", line) for line in processed_lines):\n",
    "        return processed_lines\n",
    "    else:\n",
    "        print(f\"Failed to extract array from {input_str}\")\n",
    "        return [input_str]\n",
    "\n",
    "def extract_array(input_str: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts an array from the input string.\n",
    "\n",
    "    Args:\n",
    "        input_str (str): The input string.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: The extracted array as a list of strings.\n",
    "    \"\"\"\n",
    "    regex = (\n",
    "        r\"\\[\\s*\\]|\"  # Empty array check\n",
    "        r\"(\\[(?:\\s*(?:\\\"(?:[^\\\"\\\\]|\\\\.)*\\\"|\\'(?:[^\\'\\\\]|\\\\.)*\\')\\s*,?)*\\s*\\])\"\n",
    "    )\n",
    "    match = re.search(regex, input_str)\n",
    "    if match is not None:\n",
    "        return ast.literal_eval(match[0])\n",
    "    else:\n",
    "        return handle_multiline_string(input_str)\n",
    "\n",
    "def remove_prefix(input_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes the prefix from the input string.\n",
    "\n",
    "    Args:\n",
    "        input_str (str): The input string.\n",
    "\n",
    "    Returns:\n",
    "        str: The input string with the prefix removed.\n",
    "    \"\"\"\n",
    "    prefix_pattern = (\n",
    "        r\"^(Task\\s*\\d*\\.\\s*|Task\\s*\\d*[-:]?\\s*|Step\\s*\\d*[\"\n",
    "        r\"-:]?\\s*|Step\\s*[-:]?\\s*|\\d+\\.\\s*|\\d+\\s*[-:]?\\s*|^\\.\\s*|^\\.*)\"\n",
    "    )\n",
    "    return re.sub(prefix_pattern, \"\", input_str, flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "def real_tasks_filter(input_str: str) -> bool:\n",
    "    \"\"\"\n",
    "    Filters out non-task strings.\n",
    "\n",
    "    Args:\n",
    "        input_str (str): The input string.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the input string is a task, False otherwise.\n",
    "    \"\"\"\n",
    "    no_task_regex = (\n",
    "        r\"^No( (new|further|additional|extra|other))? tasks? (is )?(\"\n",
    "        r\"required|needed|added|created|inputted).*\"\n",
    "    )\n",
    "    task_complete_regex = r\"^Task (complete|completed|finished|done|over|success).*\"\n",
    "    do_nothing_regex = r\"^(\\s*|Do nothing(\\s.*)?)$\"\n",
    "\n",
    "    return (\n",
    "        not re.search(no_task_regex, input_str, re.IGNORECASE)\n",
    "        and not re.search(task_complete_regex, input_str, re.IGNORECASE)\n",
    "        and not re.search(do_nothing_regex, input_str, re.IGNORECASE)\n",
    "    )\n",
    "\n",
    "def parse_with_handling(parser: BaseOutputParser[T], completion: str) -> T:\n",
    "    \"\"\"\n",
    "    Parses the completion using the specified parser with error handling.\n",
    "\n",
    "    Args:\n",
    "        parser (BaseOutputParser[T]): The output parser.\n",
    "        completion (str): The completion to parse.\n",
    "\n",
    "    Returns:\n",
    "        T: The parsed output.\n",
    "\n",
    "    Raises:\n",
    "        OutputParserException: If parsing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return parser.parse(completion)\n",
    "    except OutputParserException as e:\n",
    "        raise e\n",
    "\n",
    "def format_objectives(objectives: str):\n",
    "    \"\"\"\n",
    "    Formats the objectives.\n",
    "\n",
    "    Args:\n",
    "        objectives (str): The objectives to format.\n",
    "\n",
    "    Returns:\n",
    "        The formatted objectives.\n",
    "    \"\"\"\n",
    "    task_output_parser = TaskOutputParser(tasks=[])\n",
    "    objectives = parse_with_handling(task_output_parser, objectives)\n",
    "    return objectives\n",
    "\n",
    "def get_sub_objectives_list(query):\n",
    "    \"\"\"\n",
    "    Gets the list of sub-objectives for the given query.\n",
    "\n",
    "    Args:\n",
    "        query: The query to get sub-objectives for.\n",
    "\n",
    "    Returns:\n",
    "        The list of sub-objectives.\n",
    "    \"\"\"\n",
    "    sub_objectives = create_sub_objectives(query)\n",
    "    return format_objectives(sub_objectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the objective and assign an appropriate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_experts_descritpion():\n",
    "    \"\"\"\n",
    "    Retrieves the descriptions of available experts.\n",
    "\n",
    "    Returns:\n",
    "        expert_descriptions (dict): A dictionary containing expert names as keys and their descriptions as values.\n",
    "    \"\"\"\n",
    "    available_experts = get_available_experts()\n",
    "    experts = list(map(get_expert_function, available_experts))\n",
    "    expert_descriptions = {expert['name'] : expert['description'] for expert in experts}\n",
    "    return expert_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of all the expert name to expert instance\n",
    "def expertsName_to_ExpertInstance():\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping expert names to expert instances.\n",
    "\n",
    "    Returns:\n",
    "        experts (dict): A dictionary where the keys are expert names and the values are expert instances.\n",
    "    \"\"\"\n",
    "    available_experts = get_available_experts()\n",
    "    experts = {expert.name : expert for expert in available_experts}\n",
    "    return experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_objective_to_expert(query, sub_objective):\n",
    "    \"\"\"\n",
    "    Assigns the given query and sub-objective to the best expert based on their expertise.\n",
    "\n",
    "    Args:\n",
    "        query (str): The high-level objective.\n",
    "        sub_objective (str): The current task.\n",
    "\n",
    "    Returns:\n",
    "        expert_object: The selected expert object.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    expert_descriptions = get_available_experts_descritpion()\n",
    "\n",
    "    # Define the prompt template for analyzing the task\n",
    "    analyze_task_prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        High level objective: \"{query}\"\n",
    "        Current task: \"{sub_objective}\"\n",
    "\n",
    "        Based on this information, use the best expert to make progress or accomplish the task entirely. Each expert has a name and description of their expertise. \n",
    "        Select the correct expert by being smart and efficient. Ensure \"reasoning\" and only \"reasoning\" is in the english language.\n",
    "        Following are the list of experts {experts}\n",
    "\n",
    "        Note you MUST select a expert and return the name and reasoning of the expert in form of dictionary, for example \"\\ \"name\": selected_expert_name, \"reasoning\": ....\\\"\n",
    "\n",
    "        \"\"\",\n",
    "        input_variables=[\"query\", \"sub_objective\", \"experts\"],\n",
    "    )\n",
    "\n",
    "    # Format the prompt with the given query, sub-objective, and expert descriptions\n",
    "    analyze_prompt = analyze_task_prompt.format(query=query, sub_objective=sub_objective, experts=expert_descriptions)\n",
    "\n",
    "    # Invoke the language model service to analyze the task prompt\n",
    "    response = llm_service.invoke(analyze_prompt)\n",
    "\n",
    "    # Parse the selected expert from the response\n",
    "    selected_expert = json.loads(response.content)\n",
    "\n",
    "    # Get the expert object based on the selected expert's name\n",
    "    expert_object = expertsName_to_ExpertInstance().get(selected_expert['name'])\n",
    "\n",
    "    return expert_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(query):\n",
    "    \"\"\"\n",
    "    Executes the main flow of the agentic workflow.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to be processed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of results for each sub-objective.\n",
    "    \"\"\"\n",
    "    # Get the list of sub-objectives\n",
    "    sub_objectives = get_sub_objectives_list(query)\n",
    "\n",
    "    complete_response = []\n",
    "    for sub_objective in sub_objectives:\n",
    "        # Assign the objective to an expert and create an instance\n",
    "        expert = assign_objective_to_expert(query, sub_objective)\n",
    "        expert_instance = expert()\n",
    "        \n",
    "        # Call the expert's method to process the query\n",
    "        result = expert_instance.call(query)\n",
    "        \n",
    "        # Append the result to the complete response list\n",
    "        complete_response.append(result)\n",
    "    \n",
    "    print(\"All the objectives are completed successfully!\")\n",
    "    return complete_response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(query):\n",
    "    \"\"\"\n",
    "    Executes the main flow of the agentic workflow.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to be processed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of results for each sub-objective.\n",
    "    \"\"\"\n",
    "    # Get the list of sub-objectives\n",
    "    sub_objectives = get_sub_objectives_list(query)\n",
    "    print(\"Sub objectives are: \", sub_objectives)\n",
    "\n",
    "    complete_response = []\n",
    "    for sub_objective in sub_objectives:\n",
    "        print(\"Sub objective is: \", sub_objective)\n",
    "        \n",
    "        # Assign the objective to an expert and create an instance\n",
    "        expert = assign_objective_to_expert(query, sub_objective)\n",
    "        expert_instance = expert()\n",
    "        \n",
    "        # Call the expert's method to process the query\n",
    "        result = expert_instance.call(query)\n",
    "        print(\"Result is: \", result)\n",
    "        \n",
    "        # Append the result to the complete response list\n",
    "        complete_response.append(result)\n",
    "    \n",
    "    print(\"All the objectives are completed successfully!\")\n",
    "    return complete_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to extract array from - understand_metrics: Clarify the definition and purpose of the \"vessel name\" column in the dataset.\n",
      "Sub objectives are:  ['- understand_metrics: Clarify the definition and purpose of the \"vessel name\" column in the dataset.']\n",
      "Sub objective is:  - understand_metrics: Clarify the definition and purpose of the \"vessel name\" column in the dataset.\n",
      "{'name': 'MetricDiscovery', 'reasoning': \"The 'vessel name' column likely refers to the name of a ship or maritime vessel. Understanding the purpose of this column involves interpreting the data it represents, which falls under the expertise of the MetricDiscovery expert. This expert is suited for explaining the definition and purpose of specific metrics or columns in a dataset.\"}\n",
      "Telemetry method called and noop'd\n",
      "\n",
      "Telemetry method called and noop'd\n",
      "\n",
      "Telemetry method called and noop'd\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The user is asking for specific metrics that reflect the health of their business. To capture the intent accurately, I need to rephrase the query to focus on identifying the key performance indicators (KPIs) that would be used to assess the overall business health.\n",
      "\n",
      "Action: rephrase_input_query\n",
      "\n",
      "Action Input: {\"query\": \"What are the top three metrics that indicate the overall health of our business?\"}\n",
      "\u001b[0m\u001b[95m \n",
      "\n",
      "Could you identify the three most critical indicators that reflect the comprehensive well-being of our company?\n",
      "\u001b[00m\n",
      "Telemetry method called and noop'd\n",
      "\n",
      "\u001b[32;1m\u001b[1;3mThought: The rephrased query now captures the user's intent to identify the most critical indicators of the company's comprehensive well-being, which aligns with the request for metrics that indicate the overall health of the business.\n",
      "\n",
      "Final Answer: Could you identify the three most critical indicators that reflect the comprehensive well-being of our company?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To identify the three most critical indicators that reflect the comprehensive well-being of the company, I need to use the metric_discovery tool to find relevant metrics that are typically used to measure a company's overall health. These might include financial metrics, customer satisfaction indicators, employee engagement scores, or other KPIs that are commonly used to assess corporate well-being.\n",
      "\n",
      "Action: metric_discovery\n",
      "\n",
      "Action Input: {\"query\": \"three most critical indicators of company well-being\"}\n",
      "\u001b[0m\u001b[95m \n",
      "\n",
      "Sorry, the query is out of scope.\n",
      "\u001b[00m\n",
      "Telemetry method called and noop'd\n",
      "\n",
      "\u001b[32;1m\u001b[1;3mThought: Since the metric_discovery tool returned an out-of-scope response, it means that there are no relevant metrics in the metadata that match the query for the three most critical indicators of company well-being. As per the instructions, if no relevant metric exists in metadata, I must provide a specific response and terminate the process.\n",
      "\n",
      "Final Answer: Sorry, the query is out of scope.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Telemetry method called and noop'd\n",
      "\n",
      "Result is:  Sorry, the query is out of scope.\n",
      "All the objectives are completed successfully!\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the top three metrics that indicate the overall health of our business?\"\n",
    "output = main(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
