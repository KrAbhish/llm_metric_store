{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `AzureOpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureOpenAIEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_community/embeddings/azure_openai.py:113: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://polarisopenai.openai.azure.com/ to https://polarisopenai.openai.azure.com//openai.\n",
      "  warnings.warn(\n",
      "/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_community/embeddings/azure_openai.py:120: UserWarning: As of openai>=1.0.0, if `deployment` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_community/embeddings/azure_openai.py:128: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://polarisopenai.openai.azure.com/ to https://polarisopenai.openai.azure.com//openai.\n",
      "  warnings.warn(\n",
      "/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_community/chat_models/azure_openai.py:167: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://gdaopenaiswedencentral.openai.azure.com/ to https://gdaopenaiswedencentral.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_community/chat_models/azure_openai.py:174: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_community/chat_models/azure_openai.py:182: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://gdaopenaiswedencentral.openai.azure.com/ to https://gdaopenaiswedencentral.openai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.utils.llamaindex_retriever import LlamaIndexRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.azure_openai import AzureOpenAIEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "def load_env_variables(file_path):\n",
    "    load_dotenv(file_path)\n",
    "    print(\"Environment variables loaded successfully!\")\n",
    "\n",
    "env_file_path = \"../.env\"\n",
    "load_env_variables = load_env_variables(env_file_path)\n",
    "\n",
    "\n",
    "# embeddings = AzureOpenAIEmbeddings(azure_deployment=azure_deployment, openai_api_version=openai_api_version)\n",
    "embeddings =  AzureOpenAIEmbeddings(\n",
    "        deployment=os.getenv(\"EMB_DEPLOYMENT\"),\n",
    "        openai_api_version=os.getenv(\"EMB_OPENAI_API_VERSION\"),\n",
    "        model=os.getenv(\"EMB_MODEL\"),\n",
    "        openai_api_key=os.getenv(\"EMB_OPENAI_API_KEY\"),\n",
    "        openai_api_base=os.getenv(\"EMB_OPENAI_ENDPOINT\"),\n",
    "        openai_api_type=os.getenv(\"EMB_API_TYPE\"),\n",
    "    )\n",
    "\n",
    "llm_gpt = AzureChatOpenAI(deployment_name=os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME'), openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "                        openai_api_base=os.getenv(\"OPENAI_API_BASE\"), \n",
    "                        openai_api_type= os.getenv(\"OPENAI_API_TYPE\"),\n",
    "                        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                        max_tokens=3500,\n",
    "                        temperature=0.0)\n",
    "# for key, value in os.environ.items():\n",
    "#   print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.cube_semantic_custom import CubeSemanticLoader\n",
    "def fetch_cube_metadata(*args, **kwargs):\n",
    "    try:\n",
    "        # # Load document from Cube meta api\n",
    "        loader = CubeSemanticLoader(os.getenv(\"CUBE_API_URL\"), os.getenv(\"CUBE_TOKEN\"), False)\n",
    "        documents = loader.load()\n",
    "        # to_json()\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        # Handle exceptions gracefully and return an error response\n",
    "        print(\"Error in fetching metadata from cube: \" + str(e))\n",
    "        return 0\n",
    "\n",
    "def create_vector_store(documents, local_vector_store_path, *args, **kwargs):\n",
    "    print(\"Loaded documents: \" + str(documents))\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    vectorstore.save_local(local_vector_store_path)\n",
    "    print(\"Vector store created and saved successfully!\")\n",
    "\n",
    "def load_vector_store(vector_store_path, embeddings, *args, **kwargs):\n",
    "    # Load the vector store from the local file system\n",
    "    vectorstore = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"Vector store loaded successfully!\")\n",
    "    \n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "vector_store_path = \"/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/vector_store/cube_meta_faiss_index\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'FAISS' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'FAISS' has no len()"
     ]
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from http://localhost:4000/cubejs-api/v1...\n",
      "Error in fetching metadata from cube: HTTPConnectionPool(host='localhost', port=4000): Max retries exceeded with url: /cubejs-api/v1/meta (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x12f4368d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n"
     ]
    }
   ],
   "source": [
    "vector_store_path = \"/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/vector_store/cube_meta_faiss_index\"\n",
    "cube_metadata_documents = fetch_cube_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from http://localhost:4000/cubejs-api/v1...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loader = CubeSemanticLoader(os.getenv(\"CUBE_API_URL\"), os.getenv(\"CUBE_TOKEN\"), False)\n",
    "cube_metadata = loader.fetch_raw_cube_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filepath = \"/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/preloaded_cube_metadata/cube_metadata.json\"\n",
    "#save the cube metadata to a file\n",
    "with open(filepath, \"w\") as file:\n",
    "    json.dump(cube_metadata, file)\n",
    "\n",
    "print(\"Cube metadata saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the cube metadata from the file and format the data into list[Document] for further processing while using \n",
    "import json\n",
    "\n",
    "filepath = \"/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/preloaded_cube_metadata/cube_metadata.json\"\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open(filepath, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "documents = loader.format_raw_cube_metadata(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "vectorstore = load_vector_store(vector_store_path, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Controller\n",
    "from langchain import PromptTemplate\n",
    "start_goal_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an very experienced and expert manager. You are given following query \"{goal}\".\n",
    "    The query may contain different objectives which needs an expertise to solve. Your job is to provide a list of objectives that is easier for the experts to serve the query.\n",
    "    Ensure objectives are streamlined to minimize the overall number of tasks while addressing the objective effectively. \n",
    "    Some part of the query may not be relevant to the objectives. You can ignore those parts.\n",
    "    The number of tasks should be strictly less than 10.\n",
    "    \"\"\",\n",
    "    input_variables=[\"goal\", \"language\", \"tools\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools\n",
    "\n",
    "Metric Discovery: Assuming we are trying to 3 metrics \n",
    "Agent: Get relevant \n",
    "Get 3 top documents \n",
    "\n",
    "Tasks: Metric Discovery \n",
    "1. Rewrite the and Extract specifics of metric information for better retrieval\n",
    "2. Extract top 3 documents related to metric information. (Basic RAG)\n",
    "3. get information about metrics and compile the results and return \n",
    "\n",
    "Tasks: Information regarding the metric\n",
    "1. Check if metric exists\n",
    "2. Query vector database to descrive about the metric (RAG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from crewai import Agent, Task\n",
    "from langchain.tools import tool\n",
    "from src.utils.llamaindex_retriever import LlamaIndexRetriever\n",
    "#Will create below functions once the input is setup properly\n",
    "# def create_vector_store\n",
    "\n",
    "# def ingest_cube_meta():\n",
    "\n",
    "\n",
    "\n",
    "@tool(\"Extract metric queried.\")\n",
    "def extract_metric_description(query, *args, **kwargs):\n",
    "  \"\"\"Useful to isolate metric information to be further used in the its discovery\"\"\"\n",
    "  agent = Agent(\n",
    "        role='Metric Isolator',\n",
    "        goal=\n",
    "        'Accurate Extraction of Metric name or description',\n",
    "        backstory=\n",
    "        \"You're a Metric isolator which is responsible for extracting the metrics name from the given content.\",\n",
    "        llm = llm_gpt,\n",
    "        allow_delegation=False)\n",
    "  task = Task(\n",
    "        agent=agent,\n",
    "        description=\n",
    "        f' The primary objective is to ensure that all the metrics are correctly isolated and extracted from the query {query}. This extracted information should be organized in a form of list.',\n",
    "        expected_output=\"some string\",\n",
    "  \n",
    "    )\n",
    "  extracted_metrics = task.execute()\n",
    "\n",
    "  return extracted_metrics\n",
    "\n",
    "faiss_index_path = \"/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/vector_store/cube_meta_faiss_index\"\n",
    "vectorstore = FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "def get_similar_documents(query, max_number_documents=3):\n",
    "  faiss_index_path = \"/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/vector_store/cube_meta_faiss_index\"\n",
    "  vectorstore = FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "  docs = vectorstore.similarity_search_with_relevance_scores(query, max_number_documents)\n",
    "  conversational_metadata = []\n",
    "  for doc in docs:\n",
    "      doc = doc[0]\n",
    "      meta = {'text':doc.page_content, 'table_metadata': doc.metadata}\n",
    "      conversational_metadata.append(meta)\n",
    "  return conversational_metadata\n",
    "\n",
    "# @tool(\"Metric Discovery.\")\n",
    "def metric_discovery_faiss(metric_description: int, *args, **kwargs):\n",
    "  \"\"\"Useful for general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns.\"\"\"\n",
    "  # documents = fetch_cube_metadata()\n",
    "  # retriever = LlamaIndexRetriever('metric discovery')\n",
    "  # retriever.index_document(documents)\n",
    "\n",
    "  # docs = vectorstore.similarity_search_with_relevance_scores(metric_description, max_number_documents)\n",
    "  # conversational_metadata = []\n",
    "  # for doc in docs:\n",
    "  #   meta = {'text':doc.text, 'table_metadata': doc.metadata}\n",
    "  #   conversational_metadata.append(meta)\n",
    "  conversational_metadata = get_similar_documents(metric_description)\n",
    "  agent = Agent(\n",
    "        role='Data Analyst Assistant',\n",
    "        goal=\n",
    "        'Empower users to understand and utilize data effectively. This includes helping them discover relevant metrics, interpreting their meaning',\n",
    "        backstory=\n",
    "        \"The primary purpose is to bridge the gap between raw data and user comprehension, fostering a data-driven culture within the organization.\",\n",
    "        llm = llm_gpt,\n",
    "        allow_delegation=False)  \n",
    "  \n",
    "  task = Task(\n",
    "        agent=agent,\n",
    "        description=\n",
    "        \"\"\" You are responding to  question {metric_description} with answer from the Metadata provided to you as {conversational_metadata}}.\n",
    "            Strictly answer the question with the information present in metadata only.\n",
    "            Respond with \"Sorry, the query is out of scope.\" if the answer is not present in metadata.\n",
    "            \"\"\",\n",
    "        expected_output=\"some string\",\n",
    "  \n",
    "    )\n",
    "  output = task.execute()\n",
    "\n",
    "  return output\n",
    "\n",
    "@tool(\"capture all possible user intent .\")\n",
    "def capture_query_intent_possibilities(query, *args, **kwargs):\n",
    "  \"\"\"Useful to capture the intent of the user regarding metric information\"\"\"\n",
    "  agent = Agent(\n",
    "        role='Intent Capturer',\n",
    "        goal=\n",
    "        'capture the intent of the user regarding metric information',\n",
    "        backstory=\n",
    "        \"You are an expert to understand the user's intent and rephrase the query to capture the intent of the user accurately.\",\n",
    "        llm = llm_gpt,\n",
    "        allow_delegation=False)\n",
    "  task = Task(\n",
    "        agent=agent,\n",
    "        description=\n",
    "        f' The primary objective is to ensure that all the metrics are correctly isolated and extracted from the query {query}. This extracted information should be organized in a form of list.',\n",
    "        expected_output=\"some string\",\n",
    "  \n",
    "    )\n",
    "  extracted_metrics = task.execute()\n",
    "\n",
    "  return extracted_metrics\n",
    "\n",
    "@tool(\"Rephrase input query.\")\n",
    "def rephrase_input_query(query, *args, **kwargs):\n",
    "  \"\"\"Useful to rephrase the query to capture the intent of the user regarding metric information\"\"\"\n",
    "  agent = Agent(\n",
    "        role='Intent Capturer',\n",
    "        goal=\n",
    "        'Rephrasing the query to capture the intent of the user regarding metric information',\n",
    "        backstory=\n",
    "        \"You are an expert to understand the user's intent and rephrase the query to capture the intent of the user accurately.\",\n",
    "        llm = llm_gpt,\n",
    "        allow_delegation=False)\n",
    "  task = Task(\n",
    "        agent=agent,\n",
    "        description=\n",
    "        f'Rephrase the query to capture the intent of the user regarding metric information. The query is {query}. Donot add any noise to the response',\n",
    "        expected_output=\"some string\",\n",
    "  \n",
    "    )\n",
    "  extracted_metrics = task.execute()\n",
    "\n",
    "  return extracted_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response of capture_query_intent_possibilities tool\n",
    "To accurately capture the user's intent regarding metric information for the query \"What is the most popular feature used by our paid subscribers,\" the following list isolates and extracts the key metrics:\n",
    "\n",
    "1. Feature popularity ranking among paid subscribers\n",
    "2. Usage statistics of features by paid subscribers\n",
    "3. Comparison of feature usage between paid and non-paid subscribers (if applicable)\n",
    "4. Identification of the single most used feature by paid subscribers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional, Type\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "class MetricDiscoveryInput(BaseModel):\n",
    "    # model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    metric_description: str = Field(description=\"query regarding metric\")\n",
    "\n",
    "class CustomSearchTool(BaseTool):\n",
    "    name = \"custom_search\"\n",
    "    description = \"useful for when you need to answer questions about current events\"\n",
    "    args_schema: Type[BaseModel] = MetricDiscoveryInput\n",
    "\n",
    "    def _run(\n",
    "        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        output = metric_discovery_faiss(query)\n",
    "        return output\n",
    "\n",
    "    async def _arun(\n",
    "        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"custom_search does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for StructuredTool\nargs_schema\n  subclass of BaseModel expected (type=type_error.subclass; expected_class=BaseModel)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     metric_description: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m Field(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery regarding metric\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# vectorstore: Optional[FAISS] = Field(description=\"vectorstore to get indexed documents\")\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m metric_discovery \u001b[38;5;241m=\u001b[39m \u001b[43mStructuredTool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_discovery_faiss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMetric Discovery.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mUseful for general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns.\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetricDiscoveryInput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_direct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/agentgpt/lib/python3.11/site-packages/langchain_core/tools.py:845\u001b[0m, in \u001b[0;36mStructuredTool.from_function\u001b[0;34m(cls, func, coroutine, name, description, return_direct, args_schema, infer_schema, **kwargs)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _args_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m infer_schema:\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;66;03m# schema name is appended within function\u001b[39;00m\n\u001b[1;32m    844\u001b[0m     _args_schema \u001b[38;5;241m=\u001b[39m create_schema_from_function(name, source_function)\n\u001b[0;32m--> 845\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoroutine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoroutine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_args_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_direct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_direct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/agentgpt/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for StructuredTool\nargs_schema\n  subclass of BaseModel expected (type=type_error.subclass; expected_class=BaseModel)"
     ]
    }
   ],
   "source": [
    "# from langchain.tools import BaseTool, StructuredTool, tool\n",
    "# from langchain.pydantic_v1 import BaseModel, Field\n",
    "# # from langchain.pydantic_v1 import  Field\n",
    "# # from pydantic import BaseModel\n",
    "# from typing import Optional\n",
    "\n",
    "# from pydantic import BaseModel, ConfigDict, ValidationError\n",
    "\n",
    "# class MetricDiscoveryInput(BaseModel):\n",
    "#     # model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "#     metric_description: str = Field(description=\"query regarding metric\")\n",
    "#     # vectorstore: Optional[FAISS] = Field(description=\"vectorstore to get indexed documents\")\n",
    "\n",
    "# metric_discovery = StructuredTool.from_function(\n",
    "#     func=metric_discovery_faiss,\n",
    "#     name=\"Metric Discovery.\",\n",
    "#     description=\"\"\"Useful for general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns.\"\"\",\n",
    "#     args_schema=MetricDiscoveryInput,\n",
    "#     return_direct=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt_metric_discovery = f\"You are given a metric description as input {metric_description}. Your task is to determine whether this specific metric exists within the context provided. Thoroughly analyze the metric description and provide a definitive answer on the existence of the metric, along with any relevant details or insights that support your conclusion.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index_path = \"/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/vector_store/cube_meta_faiss_index\"\n",
    "vectorstore = FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "query = \"What is the most popular feature used by our paid subscribers\"\n",
    "# response = metric_discovery_faiss(query, vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, the query is out of scope.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the most popular feature used by our paid subscribers\"\n",
    "dummy_tool = CustomSearchTool()\n",
    "response = dummy_tool.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Task\n",
    "from textwrap import dedent\n",
    "\n",
    "class MetricDiscoveryTasks():\n",
    "  def metric_isolation(self, agent, query):\n",
    "    return Task(description=(f\"\"\"\n",
    "        Given query {query}, extract the description of the metric precisely.\n",
    "        Rewrite the description such that the it is more understandable to the common user.\n",
    "        \n",
    "        {self.__tip_section()}\"\"\"),\n",
    "      agent=agent,\n",
    "      expected_output=\"some string\",\n",
    "    )\n",
    "  def __tip_section(self):\n",
    "    return \"If you do your BEST WORK, I'll give you a $10,000 commission!\"\n",
    "  \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Given query Helo, I am looking for the most popular feature used by our paid subscribers, extract the description of the metric precisely.\n",
      "        Rewrite the description such that the it is more understandable to the common user.\n",
      "        \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "query =\"Helo, I am looking for the most popular feature used by our paid subscribers\"\n",
    "print(f\"\"\"\n",
    "        Given query {query}, extract the description of the metric precisely.\n",
    "        Rewrite the description such that the it is more understandable to the common user.\n",
    "        \n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent\n",
    "\n",
    "class MetricDiscoveryAgent(Agent):\n",
    "  def metric_isolation(self):\n",
    "    return Agent(\n",
    "      role=\"Extract user intent\",\n",
    "      goal=dedent(\"\"\"{Capture metric information from the user query}\"\"\"),\n",
    "      backstory=dedent(\"\"\"{You are an expert to understand and interpret what the user is asking for. You need to extract the metric information from the user query and provide a more understandable version of the metric description.}\"\"\"),\n",
    "      verbose=True,\n",
    "      tools=[\n",
    "        extract_metric_description()\n",
    "        \n",
    "      ],\n",
    "      llm = llm_gpt,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "      role=\"Extract user intent\",\n",
    "      goal=dedent(\"\"\"{Capture metric information from the user query}\"\"\"),\n",
    "      backstory=dedent(\"\"\"{You are an expert to understand and interpret what the user is asking for. You need to extract the metric information from the user query and provide a more understandable version of the metric description.}\"\"\"),\n",
    "      verbose=True,\n",
    "      tools=[\n",
    "        extract_metric_description\n",
    "      ],\n",
    "      llm = llm_gpt,\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used_tools=0 tools_errors=0 delegations=0 i18n=I18N(prompt_file=None) thread=None prompt_context=None description=\"\\n        Given query 'Helo, I am looking for the most popular feature used by our paid subscribers', extract the description of the metric precisely.\\n        Rewrite the description such that the it is more understandable to the common user.\\n        \\n        If you do your BEST WORK, I'll give you a $10,000 commission!\" expected_output='some string' config=None callback=None agent=Agent(role=Extract user intent, goal={Capture metric information from the user query}, backstory={You are an expert to understand and interpret what the user is asking for. You need to extract the metric information from the user query and provide a more understandable version of the metric description.}) context=None async_execution=False output_json=None output_pydantic=None output_file=None output=None tools=[StructuredTool(name='Extract metric queried.', description='Useful to isolate metric information to be further used in the its discovery', args_schema=<class 'pydantic.v1.main.Extract metric queried.Schema'>, func=<function extract_metric_description at 0x1390dfb00>)] id=UUID('977885b0-2a40-44b3-b6b6-5a49689cd6ca') human_input=False\n"
     ]
    }
   ],
   "source": [
    "query =\"'Helo, I am looking for the most popular feature used by our paid subscribers'\"\n",
    "tasks = MetricDiscoveryTasks()\n",
    "task = tasks.metric_isolation(agent, query)\n",
    "print(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Crew\n",
    "class MetricDiscoveryInputCrew:\n",
    "  def __init__(self, query):\n",
    "    self.query = query\n",
    "    # self.llm = llm_gpt\n",
    "\n",
    "  def run(self):\n",
    "    # agents = MetricDiscoveryAgent()\n",
    "    tasks = MetricDiscoveryTasks()\n",
    "    # print(agents)\n",
    "    print(tasks)\n",
    "    # metric_isolator_agent = agents.metric_isolation()\n",
    "    # print(metric_isolator_agent)\n",
    "    # metric_isolator_task = tasks.metric_isolation(metric_isolator_agent, self.query)\n",
    "    metric_isolator_task = tasks.metric_isolation(agent, self.query)\n",
    "    \n",
    "    print(\"Metric_isolator_task\", metric_isolator_task)\n",
    "    crew = Crew(\n",
    "      agents=[\n",
    "        # metric_isolator_agent,\n",
    "        agent\n",
    "      ],\n",
    "      tasks=[\n",
    "        metric_isolator_task\n",
    "      ],\n",
    "      verbose=False\n",
    "    )\n",
    "\n",
    "    result = crew.kickoff()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 11:31:02,746 - 140704506453952 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MetricDiscoveryTasks object at 0x13eea7750>\n",
      "Metric_isolator_task used_tools=0 tools_errors=0 delegations=0 i18n=I18N(prompt_file=None) thread=None prompt_context=None description=\"\\n        Given query What is the most popular feature used by our paid subscribers, extract the description of the metric precisely.\\n        Rewrite the description such that the it is more understandable to the common user.\\n        \\n        If you do your BEST WORK, I'll give you a $10,000 commission!\" expected_output='some string' config=None callback=None agent=Agent(role=Extract user intent, goal={Capture metric information from the user query}, backstory={You are an expert to understand and interpret what the user is asking for. You need to extract the metric information from the user query and provide a more understandable version of the metric description.}) context=None async_execution=False output_json=None output_pydantic=None output_file=None output=None tools=[StructuredTool(name='Extract metric queried.', description='Useful to isolate metric information to be further used in the its discovery', args_schema=<class 'pydantic.v1.main.Extract metric queried.Schema'>, func=<function extract_metric_description at 0x1390dfb00>)] id=UUID('a6fda851-3a5d-4790-94c9-2b1ea4e43f91') human_input=False\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to identify the specific metric information from the user's query to provide a clear and understandable description.\n",
      "\n",
      "Action: Extract metric queried.\n",
      "Action Input: {\"query\": \"What is the most popular feature used by our paid subscribers\"}\u001b[0m\u001b[95m \n",
      "\n",
      "\"most popular feature used by our paid subscribers\"\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "Final Answer: The metric is the feature among all available options that is most frequently used or engaged with by users who have an active paid subscription to the service.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the most popular feature used by our paid subscribers\"\n",
    "# formatted_query = input(\n",
    "#     dedent(\"\"\"\n",
    "#       {What is the most popular feature used by our paid subscribers}\n",
    "#     \"\"\"))\n",
    "# print(formatted_query)\n",
    "crew = MetricDiscoveryInputCrew(query)\n",
    "result = crew.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "type(formatted_query)\n",
    "\n",
    "\n",
    "print(formatted_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formtted_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectorstore.similarity_search(\"What is the most popular feature used by our paid subscribers\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"Metric Discovery.\")\n",
    "def metric_discovery_faiss(metric_description, *args, **kwargs):\n",
    "  \"\"\"Useful for general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns.\"\"\"\n",
    "  documents = fetch_cube_metadata()\n",
    "  retriever = LlamaIndexRetriever('metric discovery')\n",
    "  retriever.index_document(documents)\n",
    "\n",
    "  # rephrased_query = f\"Find the metric {metric_description}\"\n",
    "\n",
    "  response_doc = retriever.query_util(metric_description, documents)\n",
    "  conversational_metadata = []\n",
    "  for doc in response_doc:\n",
    "    meta = {'text':doc.text, 'table_metadata': doc.metadata}\n",
    "    conversational_metadata.append(meta)\n",
    "  agent = Agent(\n",
    "        role='Data Analyst Assistant',\n",
    "        goal=\n",
    "        'Empower users to understand and utilize data effectively. This includes helping them discover relevant metrics, interpreting their meaning',\n",
    "        backstory=\n",
    "        \"The primary purpose is to bridge the gap between raw data and user comprehension, fostering a data-driven culture within the organization.\",\n",
    "        llm = llm_gpt,\n",
    "        allow_delegation=False)  \n",
    "  \n",
    "  task = Task(\n",
    "        agent=agent,\n",
    "        description=\n",
    "        \"\"\" You are responding to  question {metric_description} with answer from the Metadata provided to you as {conversational_metadata}}.\n",
    "            Strictly answer the question with the information present in metadata only.\n",
    "            Respond with \"Sorry, the query is out of scope.\" if the answer is not present in metadata.\n",
    "            \"\"\",\n",
    "        expected_output=\"some string\",\n",
    "  \n",
    "    )\n",
    "  output = task.execute()\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the most popular feature used by our paid subscribers\"\n",
    "extracted_metrics = extract_metric_description(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_documents(query, vector_store):\n",
    "  docs = vector_store.similarity_search(query)\n",
    "  # take the first document as the best guess\n",
    "  table_name = docs[0].metadata[\"table_name\"]\n",
    "  # Columns\n",
    "  columns_question = \"All available columns\"\n",
    "  column_docs = vector_store.similarity_search(\n",
    "      columns_question,\n",
    "      filter=dict(table_name=table_name),\n",
    "      k=20\n",
    "  )\n",
    "  lines = []\n",
    "  for column_doc in column_docs:\n",
    "      column_title = column_doc.metadata[\"column_title\"]\n",
    "      column_name = column_doc.metadata[\"column_name\"]\n",
    "      column_data_type = column_doc.metadata[\"column_data_type\"]\n",
    "      lines.append(\n",
    "          f\"title: {column_title}, column name: {column_name}, datatype: {column_data_type}, member type: {column_doc.metadata['column_member_type']}\"\n",
    "      )\n",
    "  columns = \"\\n\".join(lines)\n",
    "  print(\"columns :\", columns)\n",
    "\n",
    "  FAISS.similarity_search_with_relevance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre process of input to make it more relevant\n",
    "import nltk\n",
    "\n",
    "# Download NLTK tokenizer data (one-time setup)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "  # Tokenization\n",
    "  tokens = nltk.word_tokenize(text.lower())\n",
    "  # Stop word removal\n",
    "  stopwords = nltk.corpus.stopwords.words('english')\n",
    "  tokens = [token for token in tokens if token not in stopwords]\n",
    "  return tokens\n",
    "\n",
    "def reformat_question(question):\n",
    "  # Define synonyms and related terms\n",
    "  metrics = [\"metric\", \"KPI\", \"measure\"]\n",
    "  sales_terms = [\"sales\", \"revenue\", \"turnover\"]\n",
    "  central_tendency = [\"average\", \"mean\", \"median\"]\n",
    "  \n",
    "  # Preprocess the question\n",
    "  preprocessed_question = preprocess_text(question)\n",
    "  \n",
    "  # Build the reformulated query string using question tokens\n",
    "  query_string = \"(\" + \" OR \".join(metrics) + \")\"\n",
    "  query_string += \" AND (\" + \" OR \".join(sales_terms) + \")\"\n",
    "  query_string += \" AND (\" + \" OR \".join(central_tendency) + \")\"\n",
    "  \n",
    "  # Include question tokens for broader coverage\n",
    "  query_string += \" AND (\" + \" OR \".join(preprocessed_question) + \")\"\n",
    "  \n",
    "  return query_string\n",
    "\n",
    "# Example usage\n",
    "question = \"Get the metric for sales on avg\"\n",
    "reformulated_question = reformat_question(question)\n",
    "print(f\"Original question: {question}\")\n",
    "print(f\"Reformatted question for FAISS search: {reformulated_question}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.langchain_retriever import LangchainRetriever\n",
    "langchain_retriever = LangchainRetriever(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = fetch_cube_metadata()\n",
    "langchain_retriever.index_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdocs = langchain_retriever.retrieve_sub_document()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loaders = [\n",
    "    TextLoader(\"/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/dummy_txt_files/file1.txt\"),\n",
    "    TextLoader(\"/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/dummy_txt_files/file2.txt\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"---\\n\\n**Shipping and Logistics Company Metrics and KPIs**\\n\\n1. **Order Accuracy Rate**\\n   - **Description**: Measures the percentage of orders correctly processed and delivered without errors.\\n   - **Formula**: (Correct Orders / Total Orders) x 100\\n   - **Importance**: High accuracy in order processing reduces returns and increases customer satisfaction.\\n\\n2. **On-Time Delivery Rate**\\n   - **Description**: Indicates the percentage of shipments delivered on or before the promised delivery date.\\n   - **Formula**: (On-Time Deliveries / Total Deliveries) x 100\\n   - **Importance**: Timely deliveries enhance customer trust and loyalty.\\n\\n3. **Average Delivery Time**\\n   - **Description**: Calculates the average time taken from order placement to delivery.\\n   - **Formula**: Total Delivery Time / Number of Deliveries\\n   - **Importance**: Helps in assessing the efficiency of the delivery process and identifying areas for improvement.\\n\\n4. **Transportation Cost per Mile**\\n   - **Description**: Measures the cost incurred per mile of transportation.\\n   - **Formula**: Total Transportation Costs / Total Miles Traveled\\n   - **Importance**: Crucial for budgeting and cost management, and helps in optimizing routes.\\n\\n5. **Freight Bill Accuracy**\\n   - **Description**: Tracks the accuracy of freight bills, ensuring there are no discrepancies between billed and actual amounts.\\n   - **Formula**: (Accurate Freight Bills / Total Freight Bills) x 100\\n   - **Importance**: Ensures proper billing and avoids financial losses due to billing errors.\\n\\n6. **Inventory Turnover Rate**\\n   - **Description**: Indicates how often inventory is sold and replaced over a period.\\n   - **Formula**: Cost of Goods Sold / Average Inventory\\n   - **Importance**: Higher turnover rates imply efficient inventory management and reduced holding costs.\\n\\n7. **Capacity Utilization**\\n   - **Description**: Measures how effectively the company's transportation and storage capacities are being utilized.\\n   - **Formula**: (Used Capacity / Total Available Capacity) x 100\\n   - **Importance**: Maximizes resource usage and identifies underutilized assets.\\n\\n8. **Customer Satisfaction Score (CSAT)**\\n   - **Description**: Gauges customer satisfaction with services provided.\\n   - **Formula**: (Total Satisfied Responses / Total Responses) x 100\\n   - **Importance**: Directly linked to customer retention and overall business growth.\\n\\n9. **Damage Rate**\\n   - **Description**: Calculates the percentage of goods damaged during transit.\\n   - **Formula**: (Damaged Goods / Total Shipped Goods) x 100\\n   - **Importance**: Identifying and reducing damage rates can significantly improve service quality and reduce costs.\\n\\n10. **Return Rate**\\n    - **Description**: Measures the percentage of delivered goods that are returned by customers.\\n    - **Formula**: (Returned Goods / Total Shipped Goods) x 100\\n    - **Importance**: High return rates may indicate issues with product quality or accuracy of orders.\\n\\n---\", metadata={'source': '/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/dummy_txt_files/file1.txt'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_retriever.index_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdocs = langchain_retriever.retrieve_sub_document(\"vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line if you need to initialize FAISS with no AVX2 optimization\n",
    "# os.environ['FAISS_NO_AVX2'] = '1'\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "import glob, os\n",
    "input_folder_path = \"/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/dummy_txt_files\"\n",
    "txt_files = glob.glob(os.path.join(input_folder_path, \"*.txt\"))\n",
    "\n",
    "loader = TextLoader(\"../../how_to/state_of_the_union.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "print(db.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/dummy_txt_files/file2.txt', '/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/dummy_txt_files/file1.txt']\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "input_folder_path = \"/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/dummy_txt_files\"\n",
    "txt_files = glob.glob(os.path.join(input_folder_path, \"*.txt\"))\n",
    "print(txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "documents = []\n",
    "for txt_file in txt_files:\n",
    "    loader = TextLoader(txt_file)\n",
    "    documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "print(db.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
