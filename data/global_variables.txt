<__main__.MetricDiscoveryTasks object at 0x139680310>
Local variables: {}
Global variables: {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', '#initialize\n#     model\n#     environment variables\n\nimport sys\nsys.path.append(\'..\')\nfrom src.utils.llamaindex_retriever import LlamaIndexRetriever\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings.azure_openai import AzureOpenAIEmbeddings\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom dotenv import load_dotenv\nimport os\nfrom langchain.chat_models import AzureChatOpenAI\n\ndef load_env_variables(file_path):\n    load_dotenv(file_path)\n    print("Environment variables loaded successfully!")\n\nenv_file_path = "../.env"\nload_env_variables = load_env_variables(env_file_path)\nmax_tokens = 3500\ntemperature = 0.1\n\n# embeddings = AzureOpenAIEmbeddings(azure_deployment=azure_deployment, openai_api_version=openai_api_version)\nembeddings =  AzureOpenAIEmbeddings(\n        deployment=os.getenv("EMB_DEPLOYMENT"),\n        openai_api_version=os.getenv("EMB_OPENAI_API_VERSION"),\n        model=os.getenv("EMB_MODEL"),\n        openai_api_key=os.getenv("EMB_OPENAI_API_KEY"),\n        openai_api_base=os.getenv("EMB_OPENAI_ENDPOINT"),\n        openai_api_type=os.getenv("EMB_API_TYPE"),\n    )\n\nllm_gpt = AzureChatOpenAI(deployment_name=os.getenv(\'AZURE_OPENAI_DEPLOYMENT_NAME\'), openai_api_version=os.getenv("OPENAI_API_VERSION"),\n                        openai_api_base=os.getenv("OPENAI_API_BASE"), \n                        openai_api_type= os.getenv("OPENAI_API_TYPE"),\n                        openai_api_key=os.getenv("OPENAI_API_KEY"),\n                        max_tokens=max_tokens,\n                        temperature=temperature)', '#define utils\nfrom src.utils.cube_semantic_custom import CubeSemanticLoader\ndef fetch_cube_metadata(*args, **kwargs):\n    try:\n        # # Load document from Cube meta api\n        loader = CubeSemanticLoader(os.getenv("CUBE_API_URL"), os.getenv("CUBE_TOKEN"), False)\n        documents = loader.load()\n        # to_json()\n        return documents\n    except Exception as e:\n        # Handle exceptions gracefully and return an error response\n        print("Error in fetching metadata from cube: " + str(e))\n        return 0\n\ndef create_vector_store(documents, local_vector_store_path, *args, **kwargs):\n    print("Loaded documents: " + str(documents))\n    vectorstore = FAISS.from_documents(documents, embeddings)\n    vectorstore.save_local(local_vector_store_path)\n    print("Vector store created and saved successfully!")\n\ndef load_vector_store(vector_store_path, embeddings, *args, **kwargs):\n    # Load the vector store from the local file system\n    vectorstore = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n    print("Vector store loaded successfully!")\n    \n    return vectorstore', '#load existing vector store\nvector_store_path = "/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/vector_store/cube_meta_faiss_index"\nvectorstore = load_vector_store(vector_store_path, embeddings)', 'import json\nfrom crewai import Agent, Task, Crew\nfrom langchain.tools import tool\nfrom src.utils.llamaindex_retriever import LlamaIndexRetriever\nfrom typing import Optional, Type\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\n# Import things that are needed generically\nfrom langchain.pydantic_v1 import BaseModel, Field\n# from langchain.tools import BaseTool, StructuredTool, tool\nfrom crewai_tools import BaseTool\n\n\ndef get_similar_documents_faiss(query, max_number_documents=3):\n  vectorstore = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n  docs = vectorstore.similarity_search_with_relevance_scores(query, max_number_documents)\n  relevant_documents = []\n  for doc in docs:\n      doc = doc[0]\n      meta = {\'text\':doc.page_content, \'table_metadata\': doc.metadata}\n      relevant_documents.append(meta)\n  return relevant_documents\n\ndef get_similar_documents(query, max_number_documents=3):\n    return get_similar_documents_faiss(query, max_number_documents)\n\n\nclass QueryInput(BaseModel):\n    query: str = Field(description="should be enquiry query")\n\n\nclass RephraseInputQuery(BaseTool):\n    name:str = "rephrase_input_query"\n    description :str = "Useful to rephrase the query to capture the intent of the user regarding metric information"\n    args_schema: Type[BaseModel] = QueryInput\n\n    def _run(\n        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n    ) -> str:\n        """Use the tool."""\n        metric_description = self.rephrase_input_query(query)\n        return metric_description\n\n    async def _arun(\n        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n    ) -> str:\n        """Use the tool asynchronously."""\n        raise NotImplementedError("custom_search does not support async")\n    \n    \n    def rephrase_input_query(self, query, *args, **kwargs):\n        agent = Agent(\n                role=\'Intent Capturer\',\n                goal=\n                \'Rephrasing the query to capture the intent of the user regarding metric information\',\n                backstory=\n                "You are an expert to understand the user\'s intent and rephrase the query to capture the intent of the user accurately.",\n                llm = llm_gpt,\n                allow_delegation=False)\n        task = Task(\n                agent=agent,\n                description=\n                f\'Rephrase the query to capture the intent of the user regarding metric information. The query is {query}. Donot add any noise to the response\',\n                expected_output="some string",\n        \n            )\n        extracted_metrics = task.execute()\n\n        return extracted_metrics\n    \n\n    \nclass MetricDiscovery(BaseTool):\n    name :str = "metric_discovery"\n    description:str = """Useful for general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns."""\n    args_schema: Type[BaseModel] = QueryInput\n\n    def _run(\n        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n    ) -> str:\n        """Use the tool."""\n        metric_description = self.metric_discovery(query)\n        return metric_description\n\n    async def _arun(\n        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n    ) -> str:\n        """Use the tool asynchronously."""\n        raise NotImplementedError("custom_search does not support async")\n    \n    \n    def metric_discovery(self, query, *args, **kwargs):\n        """Useful for general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns."""\n        relevant_documents = get_similar_documents(query)\n        agent = Agent(\n                role=\'Data Analyst Assistant\',\n                goal=\n                \'Empower users to understand and utilize data effectively. This includes helping them discover relevant metrics, interpreting their meaning\',\n                backstory=\n                "The primary purpose is to bridge the gap between raw data and user comprehension, fostering a data-driven culture within the organization.",\n                llm = llm_gpt,\n                allow_delegation=False)  \n        \n        task = Task(\n                agent=agent,\n                description=\n                """ You are responding to  question {metric_description} with answer from the Metadata provided to you as {relevant_documents}}.\n                    Strictly answer the question with the information present in metadata only.\n                    Respond with "Sorry, the query is out of scope." if the answer is not present in metadata and terminate further reasoning and prcoess.\n                    """,\n                expected_output="some string",\n        \n            )\n        output = task.execute()\n\n        return output', '# def check_termination(task_output):\n#     agent = Agent(\n#                 role=\'Determine query completion\',\n#                 goal=\n#                 \'Given task output, determine termination of crew workflow\',\n#                 backstory=\n#                 "Given user query request from user determine if no more new task is needed to be executed",\n#                 llm = llm_gpt,\n#                 allow_delegation=False)\n#     task = Task(\n#                 agent=agent,\n#                 description=\n#                 f\'Given task output, if the query is completed and the task result determines that the query is out of scope then terminate the workflow or else the required query is satisfied. Following is the task result {task_output}\',\n#                 expected_output="some string",\n        \n#             )\n#     termination_status = task.execute()\n#     return termination_status\nclass MetricDiscoveryTasks():\n  def metric_isolation(self, agent, query):\n    return Task(description=(f"""\n        Rephrase the query to capture the intent of the user regarding metric information. The query is {query}. Donot add any noise to the response.\n        {self.__tip_section()}"""),\n      agent=agent,\n      expected_output="Reformatted query to capture the intent of the user regarding metric information.",\n    )\n  \n  def metric_discovery(self, agent):\n    return Task(description=(f"""\n        Answer the general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns.\n        {self.__tip_section()}"""),\n      agent=agent,\n      expected_output="If relevant metric exists in metadata, provide the answer. Else, respond with \'Sorry, the query is out of scope.\' and terminate process.",\n    )\n  def __tip_section(self):\n    return "If you do your BEST WORK, I\'ll give you a $10,000 commission!"\n  \n  ', 'class MetricDiscoveryAgent():\n  def user_intent_capture(self):\n    return Agent(\n      role=\'Intent Capturer\',\n      goal=\n      \'Rephrasing the query to capture the intent of the user regarding metric information\',\n      backstory=\n      "You are an expert to understand the user\'s intent and rephrase the query to capture the intent of the user accurately.",\n      verbose=True,\n      tools=[\n        RephraseInputQuery()\n      ],\n      llm = llm_gpt,\n    )\n  def discover_metric_info(self):\n    return Agent(\n     role=\'Data Analyst Assistant\',\n      goal=\n      \'Empower users to understand and utilize data effectively. This includes helping them discover relevant metrics, interpreting their meaning\',\n      backstory=\n      "The primary purpose is to bridge the gap between raw data and user comprehension, fostering a data-driven culture within the organization.",\n      llm = llm_gpt,\n      verbose=True,\n      tools=[\n        MetricDiscovery()\n      ],\n    )', 'def after_task_callback(output):\n  # Perform actions after the task, \n  # for example, logging or updating agent state\n  \n  print(f"Agent completed task with result: {output}")\n\n# Assigning the function to task_callback\ntask_callback = after_task_callback', 'def test_callback():\n        # Accessing local variables\n        print("Local variables:", locals())\n        \n        # Accessing global variables\n        print("Global variables:", globals())    \n        \n        # Accessing built-in variables\n        print("Built-in variables:", dir(__builtins__))\n        after_task_callback("output")\n  ', 'from crewai.process import Process\nclass MetricDiscoveryInputCrew:\n  def __init__(self, query):\n    self.query = query\n    # self.llm = llm_gpt\n\n  def run(self):\n    agents = MetricDiscoveryAgent()\n    tasks = MetricDiscoveryTasks()\n    # print(agents)\n    print(tasks)\n    user_intent_capture_agent = agents.user_intent_capture()\n    discover_metric_info_agent = agents.discover_metric_info()\n    # print(metric_isolator_agent)\n    # metric_isolator_task = tasks.metric_isolation(metric_isolator_agent, self.query)\n    metric_isolator_task = tasks.metric_isolation(user_intent_capture_agent, self.query)\n    metric_discover_task = tasks.metric_discovery(discover_metric_info_agent)\n    \n    # print("Metric_isolator_task", metric_isolator_task)\n    crew = Crew(\n      agents=[\n        user_intent_capture_agent,\n        discover_metric_info_agent,\n      ],\n      tasks=[\n        metric_isolator_task,\n        metric_discover_task\n      ],\n      verbose=False,\n      process=Process.sequential,\n      step_callback=test_callback()\n    )\n\n    result = crew.kickoff()\n    return result', 'query = "What is the most popular feature used by our paid subscribers"\n# formatted_query = input(\n#     dedent("""\n#       {What is the most popular feature used by our paid subscribers}\n#     """))\n# print(formatted_query)\ncrew = MetricDiscoveryInputCrew(query)\nresult = crew.run()'], '_oh': {}, '_dh': [PosixPath('/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/experiments')], 'In': ['', '#initialize\n#     model\n#     environment variables\n\nimport sys\nsys.path.append(\'..\')\nfrom src.utils.llamaindex_retriever import LlamaIndexRetriever\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings.azure_openai import AzureOpenAIEmbeddings\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom dotenv import load_dotenv\nimport os\nfrom langchain.chat_models import AzureChatOpenAI\n\ndef load_env_variables(file_path):\n    load_dotenv(file_path)\n    print("Environment variables loaded successfully!")\n\nenv_file_path = "../.env"\nload_env_variables = load_env_variables(env_file_path)\nmax_tokens = 3500\ntemperature = 0.1\n\n# embeddings = AzureOpenAIEmbeddings(azure_deployment=azure_deployment, openai_api_version=openai_api_version)\nembeddings =  AzureOpenAIEmbeddings(\n        deployment=os.getenv("EMB_DEPLOYMENT"),\n        openai_api_version=os.getenv("EMB_OPENAI_API_VERSION"),\n        model=os.getenv("EMB_MODEL"),\n        openai_api_key=os.getenv("EMB_OPENAI_API_KEY"),\n        openai_api_base=os.getenv("EMB_OPENAI_ENDPOINT"),\n        openai_api_type=os.getenv("EMB_API_TYPE"),\n    )\n\nllm_gpt = AzureChatOpenAI(deployment_name=os.getenv(\'AZURE_OPENAI_DEPLOYMENT_NAME\'), openai_api_version=os.getenv("OPENAI_API_VERSION"),\n                        openai_api_base=os.getenv("OPENAI_API_BASE"), \n                        openai_api_type= os.getenv("OPENAI_API_TYPE"),\n                        openai_api_key=os.getenv("OPENAI_API_KEY"),\n                        max_tokens=max_tokens,\n                        temperature=temperature)', '#define utils\nfrom src.utils.cube_semantic_custom import CubeSemanticLoader\ndef fetch_cube_metadata(*args, **kwargs):\n    try:\n        # # Load document from Cube meta api\n        loader = CubeSemanticLoader(os.getenv("CUBE_API_URL"), os.getenv("CUBE_TOKEN"), False)\n        documents = loader.load()\n        # to_json()\n        return documents\n    except Exception as e:\n        # Handle exceptions gracefully and return an error response\n        print("Error in fetching metadata from cube: " + str(e))\n        return 0\n\ndef create_vector_store(documents, local_vector_store_path, *args, **kwargs):\n    print("Loaded documents: " + str(documents))\n    vectorstore = FAISS.from_documents(documents, embeddings)\n    vectorstore.save_local(local_vector_store_path)\n    print("Vector store created and saved successfully!")\n\ndef load_vector_store(vector_store_path, embeddings, *args, **kwargs):\n    # Load the vector store from the local file system\n    vectorstore = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n    print("Vector store loaded successfully!")\n    \n    return vectorstore', '#load existing vector store\nvector_store_path = "/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/vector_store/cube_meta_faiss_index"\nvectorstore = load_vector_store(vector_store_path, embeddings)', 'import json\nfrom crewai import Agent, Task, Crew\nfrom langchain.tools import tool\nfrom src.utils.llamaindex_retriever import LlamaIndexRetriever\nfrom typing import Optional, Type\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\n# Import things that are needed generically\nfrom langchain.pydantic_v1 import BaseModel, Field\n# from langchain.tools import BaseTool, StructuredTool, tool\nfrom crewai_tools import BaseTool\n\n\ndef get_similar_documents_faiss(query, max_number_documents=3):\n  vectorstore = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n  docs = vectorstore.similarity_search_with_relevance_scores(query, max_number_documents)\n  relevant_documents = []\n  for doc in docs:\n      doc = doc[0]\n      meta = {\'text\':doc.page_content, \'table_metadata\': doc.metadata}\n      relevant_documents.append(meta)\n  return relevant_documents\n\ndef get_similar_documents(query, max_number_documents=3):\n    return get_similar_documents_faiss(query, max_number_documents)\n\n\nclass QueryInput(BaseModel):\n    query: str = Field(description="should be enquiry query")\n\n\nclass RephraseInputQuery(BaseTool):\n    name:str = "rephrase_input_query"\n    description :str = "Useful to rephrase the query to capture the intent of the user regarding metric information"\n    args_schema: Type[BaseModel] = QueryInput\n\n    def _run(\n        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n    ) -> str:\n        """Use the tool."""\n        metric_description = self.rephrase_input_query(query)\n        return metric_description\n\n    async def _arun(\n        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n    ) -> str:\n        """Use the tool asynchronously."""\n        raise NotImplementedError("custom_search does not support async")\n    \n    \n    def rephrase_input_query(self, query, *args, **kwargs):\n        agent = Agent(\n                role=\'Intent Capturer\',\n                goal=\n                \'Rephrasing the query to capture the intent of the user regarding metric information\',\n                backstory=\n                "You are an expert to understand the user\'s intent and rephrase the query to capture the intent of the user accurately.",\n                llm = llm_gpt,\n                allow_delegation=False)\n        task = Task(\n                agent=agent,\n                description=\n                f\'Rephrase the query to capture the intent of the user regarding metric information. The query is {query}. Donot add any noise to the response\',\n                expected_output="some string",\n        \n            )\n        extracted_metrics = task.execute()\n\n        return extracted_metrics\n    \n\n    \nclass MetricDiscovery(BaseTool):\n    name :str = "metric_discovery"\n    description:str = """Useful for general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns."""\n    args_schema: Type[BaseModel] = QueryInput\n\n    def _run(\n        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n    ) -> str:\n        """Use the tool."""\n        metric_description = self.metric_discovery(query)\n        return metric_description\n\n    async def _arun(\n        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n    ) -> str:\n        """Use the tool asynchronously."""\n        raise NotImplementedError("custom_search does not support async")\n    \n    \n    def metric_discovery(self, query, *args, **kwargs):\n        """Useful for general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns."""\n        relevant_documents = get_similar_documents(query)\n        agent = Agent(\n                role=\'Data Analyst Assistant\',\n                goal=\n                \'Empower users to understand and utilize data effectively. This includes helping them discover relevant metrics, interpreting their meaning\',\n                backstory=\n                "The primary purpose is to bridge the gap between raw data and user comprehension, fostering a data-driven culture within the organization.",\n                llm = llm_gpt,\n                allow_delegation=False)  \n        \n        task = Task(\n                agent=agent,\n                description=\n                """ You are responding to  question {metric_description} with answer from the Metadata provided to you as {relevant_documents}}.\n                    Strictly answer the question with the information present in metadata only.\n                    Respond with "Sorry, the query is out of scope." if the answer is not present in metadata and terminate further reasoning and prcoess.\n                    """,\n                expected_output="some string",\n        \n            )\n        output = task.execute()\n\n        return output', '# def check_termination(task_output):\n#     agent = Agent(\n#                 role=\'Determine query completion\',\n#                 goal=\n#                 \'Given task output, determine termination of crew workflow\',\n#                 backstory=\n#                 "Given user query request from user determine if no more new task is needed to be executed",\n#                 llm = llm_gpt,\n#                 allow_delegation=False)\n#     task = Task(\n#                 agent=agent,\n#                 description=\n#                 f\'Given task output, if the query is completed and the task result determines that the query is out of scope then terminate the workflow or else the required query is satisfied. Following is the task result {task_output}\',\n#                 expected_output="some string",\n        \n#             )\n#     termination_status = task.execute()\n#     return termination_status\nclass MetricDiscoveryTasks():\n  def metric_isolation(self, agent, query):\n    return Task(description=(f"""\n        Rephrase the query to capture the intent of the user regarding metric information. The query is {query}. Donot add any noise to the response.\n        {self.__tip_section()}"""),\n      agent=agent,\n      expected_output="Reformatted query to capture the intent of the user regarding metric information.",\n    )\n  \n  def metric_discovery(self, agent):\n    return Task(description=(f"""\n        Answer the general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns.\n        {self.__tip_section()}"""),\n      agent=agent,\n      expected_output="If relevant metric exists in metadata, provide the answer. Else, respond with \'Sorry, the query is out of scope.\' and terminate process.",\n    )\n  def __tip_section(self):\n    return "If you do your BEST WORK, I\'ll give you a $10,000 commission!"\n  \n  ', 'class MetricDiscoveryAgent():\n  def user_intent_capture(self):\n    return Agent(\n      role=\'Intent Capturer\',\n      goal=\n      \'Rephrasing the query to capture the intent of the user regarding metric information\',\n      backstory=\n      "You are an expert to understand the user\'s intent and rephrase the query to capture the intent of the user accurately.",\n      verbose=True,\n      tools=[\n        RephraseInputQuery()\n      ],\n      llm = llm_gpt,\n    )\n  def discover_metric_info(self):\n    return Agent(\n     role=\'Data Analyst Assistant\',\n      goal=\n      \'Empower users to understand and utilize data effectively. This includes helping them discover relevant metrics, interpreting their meaning\',\n      backstory=\n      "The primary purpose is to bridge the gap between raw data and user comprehension, fostering a data-driven culture within the organization.",\n      llm = llm_gpt,\n      verbose=True,\n      tools=[\n        MetricDiscovery()\n      ],\n    )', 'def after_task_callback(output):\n  # Perform actions after the task, \n  # for example, logging or updating agent state\n  \n  print(f"Agent completed task with result: {output}")\n\n# Assigning the function to task_callback\ntask_callback = after_task_callback', 'def test_callback():\n        # Accessing local variables\n        print("Local variables:", locals())\n        \n        # Accessing global variables\n        print("Global variables:", globals())    \n        \n        # Accessing built-in variables\n        print("Built-in variables:", dir(__builtins__))\n        after_task_callback("output")\n  ', 'from crewai.process import Process\nclass MetricDiscoveryInputCrew:\n  def __init__(self, query):\n    self.query = query\n    # self.llm = llm_gpt\n\n  def run(self):\n    agents = MetricDiscoveryAgent()\n    tasks = MetricDiscoveryTasks()\n    # print(agents)\n    print(tasks)\n    user_intent_capture_agent = agents.user_intent_capture()\n    discover_metric_info_agent = agents.discover_metric_info()\n    # print(metric_isolator_agent)\n    # metric_isolator_task = tasks.metric_isolation(metric_isolator_agent, self.query)\n    metric_isolator_task = tasks.metric_isolation(user_intent_capture_agent, self.query)\n    metric_discover_task = tasks.metric_discovery(discover_metric_info_agent)\n    \n    # print("Metric_isolator_task", metric_isolator_task)\n    crew = Crew(\n      agents=[\n        user_intent_capture_agent,\n        discover_metric_info_agent,\n      ],\n      tasks=[\n        metric_isolator_task,\n        metric_discover_task\n      ],\n      verbose=False,\n      process=Process.sequential,\n      step_callback=test_callback()\n    )\n\n    result = crew.kickoff()\n    return result', 'query = "What is the most popular feature used by our paid subscribers"\n# formatted_query = input(\n#     dedent("""\n#       {What is the most popular feature used by our paid subscribers}\n#     """))\n# print(formatted_query)\ncrew = MetricDiscoveryInputCrew(query)\nresult = crew.run()'], 'Out': {}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x10dcd2910>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x10dce6950>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x10dce6950>, 'open': <function open at 0x10c9f5440>, '_': '', '__': '', '___': '', '__vsc_ipynb_file__': '/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/experiments/agent_flow_v2.ipynb', '_i': 'from crewai.process import Process\nclass MetricDiscoveryInputCrew:\n  def __init__(self, query):\n    self.query = query\n    # self.llm = llm_gpt\n\n  def run(self):\n    agents = MetricDiscoveryAgent()\n    tasks = MetricDiscoveryTasks()\n    # print(agents)\n    print(tasks)\n    user_intent_capture_agent = agents.user_intent_capture()\n    discover_metric_info_agent = agents.discover_metric_info()\n    # print(metric_isolator_agent)\n    # metric_isolator_task = tasks.metric_isolation(metric_isolator_agent, self.query)\n    metric_isolator_task = tasks.metric_isolation(user_intent_capture_agent, self.query)\n    metric_discover_task = tasks.metric_discovery(discover_metric_info_agent)\n    \n    # print("Metric_isolator_task", metric_isolator_task)\n    crew = Crew(\n      agents=[\n        user_intent_capture_agent,\n        discover_metric_info_agent,\n      ],\n      tasks=[\n        metric_isolator_task,\n        metric_discover_task\n      ],\n      verbose=False,\n      process=Process.sequential,\n      step_callback=test_callback()\n    )\n\n    result = crew.kickoff()\n    return result', '_ii': 'def test_callback():\n        # Accessing local variables\n        print("Local variables:", locals())\n        \n        # Accessing global variables\n        print("Global variables:", globals())    \n        \n        # Accessing built-in variables\n        print("Built-in variables:", dir(__builtins__))\n        after_task_callback("output")\n  ', '_iii': 'def after_task_callback(output):\n  # Perform actions after the task, \n  # for example, logging or updating agent state\n  \n  print(f"Agent completed task with result: {output}")\n\n# Assigning the function to task_callback\ntask_callback = after_task_callback', '_i1': '#initialize\n#     model\n#     environment variables\n\nimport sys\nsys.path.append(\'..\')\nfrom src.utils.llamaindex_retriever import LlamaIndexRetriever\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings.azure_openai import AzureOpenAIEmbeddings\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom dotenv import load_dotenv\nimport os\nfrom langchain.chat_models import AzureChatOpenAI\n\ndef load_env_variables(file_path):\n    load_dotenv(file_path)\n    print("Environment variables loaded successfully!")\n\nenv_file_path = "../.env"\nload_env_variables = load_env_variables(env_file_path)\nmax_tokens = 3500\ntemperature = 0.1\n\n# embeddings = AzureOpenAIEmbeddings(azure_deployment=azure_deployment, openai_api_version=openai_api_version)\nembeddings =  AzureOpenAIEmbeddings(\n        deployment=os.getenv("EMB_DEPLOYMENT"),\n        openai_api_version=os.getenv("EMB_OPENAI_API_VERSION"),\n        model=os.getenv("EMB_MODEL"),\n        openai_api_key=os.getenv("EMB_OPENAI_API_KEY"),\n        openai_api_base=os.getenv("EMB_OPENAI_ENDPOINT"),\n        openai_api_type=os.getenv("EMB_API_TYPE"),\n    )\n\nllm_gpt = AzureChatOpenAI(deployment_name=os.getenv(\'AZURE_OPENAI_DEPLOYMENT_NAME\'), openai_api_version=os.getenv("OPENAI_API_VERSION"),\n                        openai_api_base=os.getenv("OPENAI_API_BASE"), \n                        openai_api_type= os.getenv("OPENAI_API_TYPE"),\n                        openai_api_key=os.getenv("OPENAI_API_KEY"),\n                        max_tokens=max_tokens,\n                        temperature=temperature)', 'sys': <module 'sys' (built-in)>, 'LlamaIndexRetriever': <class 'src.utils.llamaindex_retriever.LlamaIndexRetriever'>, 'FAISS': <class 'langchain_community.vectorstores.faiss.FAISS'>, 'AzureOpenAIEmbeddings': <class 'langchain_community.embeddings.azure_openai.AzureOpenAIEmbeddings'>, 'OpenAIEmbeddings': <class 'langchain_community.embeddings.openai.OpenAIEmbeddings'>, 'load_dotenv': <function load_dotenv at 0x139004d60>, 'os': <module 'os' (frozen)>, 'AzureChatOpenAI': <class 'langchain_community.chat_models.azure_openai.AzureChatOpenAI'>, 'load_env_variables': None, 'env_file_path': '../.env', 'max_tokens': 3500, 'temperature': 0.1, 'embeddings': AzureOpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x1390e6510>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x13919c690>, model='text-embedding-ada-002', deployment=None, openai_api_version='2023-03-15-preview', openai_api_base='https://polarisopenai.openai.azure.com//openai/deployments/text-embedding-ada-002', openai_api_type='azure', openai_proxy='', embedding_ctx_length=8191, openai_api_key='35431d6cd1874798b9ebbd6bdfbb0720', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=16, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, azure_endpoint=None, azure_ad_token=None, azure_ad_token_provider=None, validate_base_url=True), 'llm_gpt': AzureChatOpenAI(callbacks=[<crewai.utilities.token_counter_callback.TokenCalcHandler object at 0x13967e190>], client=<openai.resources.chat.completions.Completions object at 0x138b85510>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1391bb0d0>, temperature=0.1, openai_api_key='43b8528ab70d48359744cd0165a1167e', openai_api_base='https://polariseastus2.openai.azure.com/openai/deployments/gpt4-turbo', openai_proxy='', max_tokens=3500, openai_api_version='2023-07-01-preview', openai_api_type='azure'), '_i2': '#define utils\nfrom src.utils.cube_semantic_custom import CubeSemanticLoader\ndef fetch_cube_metadata(*args, **kwargs):\n    try:\n        # # Load document from Cube meta api\n        loader = CubeSemanticLoader(os.getenv("CUBE_API_URL"), os.getenv("CUBE_TOKEN"), False)\n        documents = loader.load()\n        # to_json()\n        return documents\n    except Exception as e:\n        # Handle exceptions gracefully and return an error response\n        print("Error in fetching metadata from cube: " + str(e))\n        return 0\n\ndef create_vector_store(documents, local_vector_store_path, *args, **kwargs):\n    print("Loaded documents: " + str(documents))\n    vectorstore = FAISS.from_documents(documents, embeddings)\n    vectorstore.save_local(local_vector_store_path)\n    print("Vector store created and saved successfully!")\n\ndef load_vector_store(vector_store_path, embeddings, *args, **kwargs):\n    # Load the vector store from the local file system\n    vectorstore = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n    print("Vector store loaded successfully!")\n    \n    return vectorstore', 'CubeSemanticLoader': <class 'src.utils.cube_semantic_custom.CubeSemanticLoader'>, 'fetch_cube_metadata': <function fetch_cube_metadata at 0x1391a77e0>, 'create_vector_store': <function create_vector_store at 0x1391a7a60>, 'load_vector_store': <function load_vector_store at 0x1391a7c40>, '_i3': '#load existing vector store\nvector_store_path = "/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/vector_store/cube_meta_faiss_index"\nvectorstore = load_vector_store(vector_store_path, embeddings)', 'vector_store_path': '/Users/k.abhishek/Documents/experiments/metric_store/metric_store_gen_ai/data/vector_store/cube_meta_faiss_index', 'vectorstore': <langchain_community.vectorstores.faiss.FAISS object at 0x10dd38c90>, '_i4': 'import json\nfrom crewai import Agent, Task, Crew\nfrom langchain.tools import tool\nfrom src.utils.llamaindex_retriever import LlamaIndexRetriever\nfrom typing import Optional, Type\nfrom langchain.callbacks.manager import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\n# Import things that are needed generically\nfrom langchain.pydantic_v1 import BaseModel, Field\n# from langchain.tools import BaseTool, StructuredTool, tool\nfrom crewai_tools import BaseTool\n\n\ndef get_similar_documents_faiss(query, max_number_documents=3):\n  vectorstore = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n  docs = vectorstore.similarity_search_with_relevance_scores(query, max_number_documents)\n  relevant_documents = []\n  for doc in docs:\n      doc = doc[0]\n      meta = {\'text\':doc.page_content, \'table_metadata\': doc.metadata}\n      relevant_documents.append(meta)\n  return relevant_documents\n\ndef get_similar_documents(query, max_number_documents=3):\n    return get_similar_documents_faiss(query, max_number_documents)\n\n\nclass QueryInput(BaseModel):\n    query: str = Field(description="should be enquiry query")\n\n\nclass RephraseInputQuery(BaseTool):\n    name:str = "rephrase_input_query"\n    description :str = "Useful to rephrase the query to capture the intent of the user regarding metric information"\n    args_schema: Type[BaseModel] = QueryInput\n\n    def _run(\n        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n    ) -> str:\n        """Use the tool."""\n        metric_description = self.rephrase_input_query(query)\n        return metric_description\n\n    async def _arun(\n        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n    ) -> str:\n        """Use the tool asynchronously."""\n        raise NotImplementedError("custom_search does not support async")\n    \n    \n    def rephrase_input_query(self, query, *args, **kwargs):\n        agent = Agent(\n                role=\'Intent Capturer\',\n                goal=\n                \'Rephrasing the query to capture the intent of the user regarding metric information\',\n                backstory=\n                "You are an expert to understand the user\'s intent and rephrase the query to capture the intent of the user accurately.",\n                llm = llm_gpt,\n                allow_delegation=False)\n        task = Task(\n                agent=agent,\n                description=\n                f\'Rephrase the query to capture the intent of the user regarding metric information. The query is {query}. Donot add any noise to the response\',\n                expected_output="some string",\n        \n            )\n        extracted_metrics = task.execute()\n\n        return extracted_metrics\n    \n\n    \nclass MetricDiscovery(BaseTool):\n    name :str = "metric_discovery"\n    description:str = """Useful for general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns."""\n    args_schema: Type[BaseModel] = QueryInput\n\n    def _run(\n        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n    ) -> str:\n        """Use the tool."""\n        metric_description = self.metric_discovery(query)\n        return metric_description\n\n    async def _arun(\n        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n    ) -> str:\n        """Use the tool asynchronously."""\n        raise NotImplementedError("custom_search does not support async")\n    \n    \n    def metric_discovery(self, query, *args, **kwargs):\n        """Useful for general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns."""\n        relevant_documents = get_similar_documents(query)\n        agent = Agent(\n                role=\'Data Analyst Assistant\',\n                goal=\n                \'Empower users to understand and utilize data effectively. This includes helping them discover relevant metrics, interpreting their meaning\',\n                backstory=\n                "The primary purpose is to bridge the gap between raw data and user comprehension, fostering a data-driven culture within the organization.",\n                llm = llm_gpt,\n                allow_delegation=False)  \n        \n        task = Task(\n                agent=agent,\n                description=\n                """ You are responding to  question {metric_description} with answer from the Metadata provided to you as {relevant_documents}}.\n                    Strictly answer the question with the information present in metadata only.\n                    Respond with "Sorry, the query is out of scope." if the answer is not present in metadata and terminate further reasoning and prcoess.\n                    """,\n                expected_output="some string",\n        \n            )\n        output = task.execute()\n\n        return output', 'json': <module 'json' from '/Users/k.abhishek/anaconda3/envs/agentgpt/lib/python3.11/json/__init__.py'>, 'Agent': <class 'crewai.agent.Agent'>, 'Task': <class 'crewai.task.Task'>, 'Crew': <class 'crewai.crew.Crew'>, 'tool': <function tool at 0x13965b9c0>, 'Optional': typing.Optional, 'Type': typing.Type, 'AsyncCallbackManagerForToolRun': <class 'langchain_core.callbacks.manager.AsyncCallbackManagerForToolRun'>, 'CallbackManagerForToolRun': <class 'langchain_core.callbacks.manager.CallbackManagerForToolRun'>, 'BaseModel': <class 'pydantic.v1.main.BaseModel'>, 'Field': <function Field at 0x10ddc6980>, 'BaseTool': <class 'crewai_tools.tools.base_tool.BaseTool'>, 'get_similar_documents_faiss': <function get_similar_documents_faiss at 0x13965ae80>, 'get_similar_documents': <function get_similar_documents at 0x1391e1760>, 'QueryInput': <class '__main__.QueryInput'>, 'RephraseInputQuery': <class '__main__.RephraseInputQuery'>, 'MetricDiscovery': <class '__main__.MetricDiscovery'>, '_i5': '# def check_termination(task_output):\n#     agent = Agent(\n#                 role=\'Determine query completion\',\n#                 goal=\n#                 \'Given task output, determine termination of crew workflow\',\n#                 backstory=\n#                 "Given user query request from user determine if no more new task is needed to be executed",\n#                 llm = llm_gpt,\n#                 allow_delegation=False)\n#     task = Task(\n#                 agent=agent,\n#                 description=\n#                 f\'Given task output, if the query is completed and the task result determines that the query is out of scope then terminate the workflow or else the required query is satisfied. Following is the task result {task_output}\',\n#                 expected_output="some string",\n        \n#             )\n#     termination_status = task.execute()\n#     return termination_status\nclass MetricDiscoveryTasks():\n  def metric_isolation(self, agent, query):\n    return Task(description=(f"""\n        Rephrase the query to capture the intent of the user regarding metric information. The query is {query}. Donot add any noise to the response.\n        {self.__tip_section()}"""),\n      agent=agent,\n      expected_output="Reformatted query to capture the intent of the user regarding metric information.",\n    )\n  \n  def metric_discovery(self, agent):\n    return Task(description=(f"""\n        Answer the general user questions related to discovery, explaination, description, interpretation of metrics/measures/KPIs, tables or columns.\n        {self.__tip_section()}"""),\n      agent=agent,\n      expected_output="If relevant metric exists in metadata, provide the answer. Else, respond with \'Sorry, the query is out of scope.\' and terminate process.",\n    )\n  def __tip_section(self):\n    return "If you do your BEST WORK, I\'ll give you a $10,000 commission!"\n  \n  ', 'MetricDiscoveryTasks': <class '__main__.MetricDiscoveryTasks'>, '_i6': 'class MetricDiscoveryAgent():\n  def user_intent_capture(self):\n    return Agent(\n      role=\'Intent Capturer\',\n      goal=\n      \'Rephrasing the query to capture the intent of the user regarding metric information\',\n      backstory=\n      "You are an expert to understand the user\'s intent and rephrase the query to capture the intent of the user accurately.",\n      verbose=True,\n      tools=[\n        RephraseInputQuery()\n      ],\n      llm = llm_gpt,\n    )\n  def discover_metric_info(self):\n    return Agent(\n     role=\'Data Analyst Assistant\',\n      goal=\n      \'Empower users to understand and utilize data effectively. This includes helping them discover relevant metrics, interpreting their meaning\',\n      backstory=\n      "The primary purpose is to bridge the gap between raw data and user comprehension, fostering a data-driven culture within the organization.",\n      llm = llm_gpt,\n      verbose=True,\n      tools=[\n        MetricDiscovery()\n      ],\n    )', 'MetricDiscoveryAgent': <class '__main__.MetricDiscoveryAgent'>, '_i7': 'def after_task_callback(output):\n  # Perform actions after the task, \n  # for example, logging or updating agent state\n  \n  print(f"Agent completed task with result: {output}")\n\n# Assigning the function to task_callback\ntask_callback = after_task_callback', 'after_task_callback': <function after_task_callback at 0x1396980e0>, 'task_callback': <function after_task_callback at 0x1396980e0>, '_i8': 'def test_callback():\n        # Accessing local variables\n        print("Local variables:", locals())\n        \n        # Accessing global variables\n        print("Global variables:", globals())    \n        \n        # Accessing built-in variables\n        print("Built-in variables:", dir(__builtins__))\n        after_task_callback("output")\n  ', 'test_callback': <function test_callback at 0x139698360>, '_i9': 'from crewai.process import Process\nclass MetricDiscoveryInputCrew:\n  def __init__(self, query):\n    self.query = query\n    # self.llm = llm_gpt\n\n  def run(self):\n    agents = MetricDiscoveryAgent()\n    tasks = MetricDiscoveryTasks()\n    # print(agents)\n    print(tasks)\n    user_intent_capture_agent = agents.user_intent_capture()\n    discover_metric_info_agent = agents.discover_metric_info()\n    # print(metric_isolator_agent)\n    # metric_isolator_task = tasks.metric_isolation(metric_isolator_agent, self.query)\n    metric_isolator_task = tasks.metric_isolation(user_intent_capture_agent, self.query)\n    metric_discover_task = tasks.metric_discovery(discover_metric_info_agent)\n    \n    # print("Metric_isolator_task", metric_isolator_task)\n    crew = Crew(\n      agents=[\n        user_intent_capture_agent,\n        discover_metric_info_agent,\n      ],\n      tasks=[\n        metric_isolator_task,\n        metric_discover_task\n      ],\n      verbose=False,\n      process=Process.sequential,\n      step_callback=test_callback()\n    )\n\n    result = crew.kickoff()\n    return result', 'Process': <enum 'Process'>, 'MetricDiscoveryInputCrew': <class '__main__.MetricDiscoveryInputCrew'>, '_i10': 'query = "What is the most popular feature used by our paid subscribers"\n# formatted_query = input(\n#     dedent("""\n#       {What is the most popular feature used by our paid subscribers}\n#     """))\n# print(formatted_query)\ncrew = MetricDiscoveryInputCrew(query)\nresult = crew.run()', 'query': 'What is the most popular feature used by our paid subscribers', 'crew': <__main__.MetricDiscoveryInputCrew object at 0x139680c10>}
Built-in variables: ['ArithmeticError', 'AssertionError', 'AttributeError', 'BaseException', 'BaseExceptionGroup', 'BlockingIOError', 'BrokenPipeError', 'BufferError', 'BytesWarning', 'ChildProcessError', 'ConnectionAbortedError', 'ConnectionError', 'ConnectionRefusedError', 'ConnectionResetError', 'DeprecationWarning', 'EOFError', 'Ellipsis', 'EncodingWarning', 'EnvironmentError', 'Exception', 'ExceptionGroup', 'False', 'FileExistsError', 'FileNotFoundError', 'FloatingPointError', 'FutureWarning', 'GeneratorExit', 'IOError', 'ImportError', 'ImportWarning', 'IndentationError', 'IndexError', 'InterruptedError', 'IsADirectoryError', 'KeyError', 'KeyboardInterrupt', 'LookupError', 'MemoryError', 'ModuleNotFoundError', 'NameError', 'None', 'NotADirectoryError', 'NotImplemented', 'NotImplementedError', 'OSError', 'OverflowError', 'PendingDeprecationWarning', 'PermissionError', 'ProcessLookupError', 'RecursionError', 'ReferenceError', 'ResourceWarning', 'RuntimeError', 'RuntimeWarning', 'StopAsyncIteration', 'StopIteration', 'SyntaxError', 'SyntaxWarning', 'SystemError', 'SystemExit', 'TabError', 'TimeoutError', 'True', 'TypeError', 'UnboundLocalError', 'UnicodeDecodeError', 'UnicodeEncodeError', 'UnicodeError', 'UnicodeTranslateError', 'UnicodeWarning', 'UserWarning', 'ValueError', 'Warning', 'ZeroDivisionError', '__IPYTHON__', '__build_class__', '__debug__', '__doc__', '__import__', '__loader__', '__name__', '__package__', '__pybind11_internals_v4_clang_libcpp_cxxabi1002__', '__spec__', 'abs', 'aiter', 'all', 'anext', 'any', 'ascii', 'bin', 'bool', 'breakpoint', 'bytearray', 'bytes', 'callable', 'chr', 'classmethod', 'compile', 'complex', 'copyright', 'credits', 'delattr', 'dict', 'dir', 'display', 'divmod', 'enumerate', 'eval', 'exec', 'execfile', 'filter', 'float', 'format', 'frozenset', 'get_ipython', 'getattr', 'globals', 'hasattr', 'hash', 'help', 'hex', 'id', 'input', 'int', 'isinstance', 'issubclass', 'iter', 'len', 'license', 'list', 'locals', 'map', 'max', 'memoryview', 'min', 'next', 'object', 'oct', 'open', 'ord', 'pow', 'print', 'property', 'range', 'repr', 'reversed', 'round', 'runfile', 'set', 'setattr', 'slice', 'sorted', 'staticmethod', 'str', 'sum', 'super', 'tuple', 'type', 'vars', 'zip']
Agent completed task with result: output


[1m> Entering new CrewAgentExecutor chain...[0m
